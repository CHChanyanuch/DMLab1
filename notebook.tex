
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{DM19-Lab1-Master}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{data-mining-lab-1}{%
\section{Data Mining Lab 1}\label{data-mining-lab-1}}

In this lab session we will focus on the use of scientific computing
libraries to efficiently process, transform, and manage data.
Furthermore, we will provide best practices and introduce visualization
tools for effectively conducting big data analysis and visualization.

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{table-of-contents}{%
\subsection{Table of Contents}\label{table-of-contents}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Data Source
\item
  Data Preparation
\item
  Data Transformation
\end{enumerate}

\begin{itemize}
\tightlist
\item
  3.1 Converting Dictionary into Pandas dataframe
\item
  3.2 Familiarizing yourself with the Data
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Data Mining using Pandas
\end{enumerate}

\begin{itemize}
\tightlist
\item
  4.1 Dealing with Missing Values
\item
  4.2 Dealing with Duplicate Data
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Data Preprocessing
\end{enumerate}

\begin{itemize}
\tightlist
\item
  5.1 Sampling
\item
  5.2 Feature Creation
\item
  5.3 Feature Subset Selection
\item
  5.4 Dimensionality Reduction
\item
  5.5 Atrribute Transformation / Aggregation
\item
  5.6 Discretization and Binarization
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Data Exploration
\item
  Conclusion
\item
  References
\end{enumerate}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{introduction}{%
\subsection{Introduction}\label{introduction}}

In this notebook I will explore a text-based, document-based
\href{http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html}{dataset}
using scientific computing tools such as Pandas and Numpy. In addition,
several fundamental Data Mining concepts will be explored and explained
in details, ranging from calculating distance measures to computing term
frequency vectors. Coding examples, visualizations and demonstrations
will be provided where necessary. Furthermore, additional exercises are
provided after special topics. These exercises are geared towards
testing the proficiency of students and motivate students to explore
beyond the techniques covered in the notebook.

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{requirements}{%
\subsubsection{Requirements}\label{requirements}}

Here are the computing and software requirements

\hypertarget{computing-resources}{%
\paragraph{Computing Resources}\label{computing-resources}}

\begin{itemize}
\tightlist
\item
  Operating system: Preferably Linux or MacOS
\item
  RAM: 8 GB
\item
  Disk space: Mininium 8 GB
\end{itemize}

\hypertarget{software-requirements}{%
\paragraph{Software Requirements}\label{software-requirements}}

Here is a list of the required programs and libraries necessary for this
lab session:

\hypertarget{language}{%
\subparagraph{Language:}\label{language}}

\begin{itemize}
\tightlist
\item
  \href{https://www.python.org/download/releases/3.0/}{Python 3+} (Note:
  coding will be done strictly on Python 3)

  \begin{itemize}
  \tightlist
  \item
    Install latest version of Python 3
  \end{itemize}
\end{itemize}

\hypertarget{environment}{%
\subparagraph{Environment:}\label{environment}}

Using an environment is to avoid some library conflict problems. You can
refer this \href{http://cs231n.github.io/setup-instructions/}{Setup
Instructions} to install and setup.

\begin{itemize}
\tightlist
\item
  \href{https://www.anaconda.com/download/}{Anaconda} (recommended but
  not required)

  \begin{itemize}
  \tightlist
  \item
    Install anaconda environment
  \end{itemize}
\item
  \href{https://virtualenv.pypa.io/en/stable/userguide/}{Python
  virtualenv} (recommended to Linux/MacOS user)

  \begin{itemize}
  \tightlist
  \item
    Install virtual environment
  \end{itemize}
\item
  \href{https://www.kaggle.com/kernels/}{Kaggle Kernel}

  \begin{itemize}
  \tightlist
  \item
    Run on the cloud (with some limitations)
  \item
    Reference:
    \href{https://github.com/omarsar/data_mining_lab/blob/master/kagglekernel.md}{Kaggle
    Kernels Instructions}
  \end{itemize}
\end{itemize}

\hypertarget{necessary-libraries}{%
\subparagraph{Necessary Libraries:}\label{necessary-libraries}}

\begin{itemize}
\tightlist
\item
  \href{http://jupyter.org/}{Jupyter} (Strongly recommended but not
  required)

  \begin{itemize}
  \tightlist
  \item
    Install \texttt{jupyter} and Use \texttt{\$jupyter\ notebook} in
    terminal to run
  \end{itemize}
\item
  \href{http://scikit-learn.org/stable/index.html}{Scikit Learn}

  \begin{itemize}
  \tightlist
  \item
    Install \texttt{sklearn} latest python library
  \end{itemize}
\item
  \href{http://pandas.pydata.org/}{Pandas}

  \begin{itemize}
  \tightlist
  \item
    Install \texttt{pandas} python library
  \end{itemize}
\item
  \href{http://www.numpy.org/}{Numpy}

  \begin{itemize}
  \tightlist
  \item
    Install \texttt{numpy} python library
  \end{itemize}
\item
  \href{https://matplotlib.org/}{Matplotlib}

  \begin{itemize}
  \tightlist
  \item
    Install \texttt{maplotlib} for python
  \end{itemize}
\item
  \href{https://plot.ly/}{Plotly}

  \begin{itemize}
  \tightlist
  \item
    Install and signup for \texttt{plotly}
  \end{itemize}
\item
  \href{https://seaborn.pydata.org/}{Seaborn}

  \begin{itemize}
  \tightlist
  \item
    Install and signup for \texttt{seaborn}
  \end{itemize}
\item
  \href{http://www.nltk.org/}{NLTK}

  \begin{itemize}
  \tightlist
  \item
    Install \texttt{nltk} library
  \end{itemize}
\end{itemize}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{c+c1}{\PYZsh{} necessary for when working with external scripts}
        \PY{o}{\PYZpc{}}\PY{k}{load\PYZus{}ext} autoreload
        \PY{o}{\PYZpc{}}\PY{k}{autoreload} 2
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{the-data}{%
\subsection{1. The Data}\label{the-data}}

In this notebook we will explore the popular 20 newsgroup dataset,
originally provided \href{http://qwone.com/~jason/20Newsgroups/}{here}.
The dataset is called ``Twenty Newsgroups'', which means there are 20
categories of news articles available in the entire dataset. A short
description of the dataset, provided by the authors, is provided below:

\begin{itemize}
\tightlist
\item
  \emph{The 20 Newsgroups data set is a collection of approximately
  20,000 newsgroup documents, partitioned (nearly) evenly across 20
  different newsgroups. To the best of our knowledge, it was originally
  collected by Ken Lang, probably for his paper ``Newsweeder: Learning
  to filter netnews,'' though he does not explicitly mention this
  collection. The 20 newsgroups collection has become a popular data set
  for experiments in text applications of machine learning techniques,
  such as text classification and text clustering.}
\end{itemize}

If you need more information about the dataset please refer to the
reference provided above. Below is a snapshot of the dataset already
converted into a table. Keep in mind that the original dataset is not in
this nice pretty format. That work is left to us. That is one of the
tasks that will be covered in this notebook: how to convert raw data
into convenient tabular formats using Pandas.

\begin{figure}
\centering
\includegraphics{https://docs.google.com/drawings/d/e/2PACX-1vRd845nNXa1x1Enw6IoEbg-05lB19xG3mfO2BjnpZrloT0pSnY89stBV1gS9Iu6cgRCTq3E5giIT5ZI/pub?w=835\&h=550}
\caption{atl txt}
\end{figure}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{data-preparation}{%
\subsection{2. Data Preparation}\label{data-preparation}}

Now let us begin to explore the data. The original dataset can be found
on the link provided above or you can directly use the version provided
by scikit learn. Here we will use the scikit learn version.

In this demonstration we are only going to look at 4 categories. This
means we will not make use of the complete dataset, but only a subset of
it, which includes the 4 categories defined below:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{c+c1}{\PYZsh{} categories}
        \PY{n}{categories} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{alt.atheism}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{soc.religion.christian}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{comp.graphics}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sci.med}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{c+c1}{\PYZsh{} obtain the documents containing the categories provided}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{datasets} \PY{k}{import} \PY{n}{fetch\PYZus{}20newsgroups}
        
        \PY{n}{twenty\PYZus{}train} \PY{o}{=} \PY{n}{fetch\PYZus{}20newsgroups}\PY{p}{(}\PY{n}{subset}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{categories}\PY{o}{=}\PY{n}{categories}\PY{p}{,} \PYZbs{}
                                          \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{)}
\end{Verbatim}


    Let's take at look some of the records that are contained in our subset
of the data

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{n}{twenty\PYZus{}train}\PY{o}{.}\PY{n}{data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}4}]:} ['From: sd345@city.ac.uk (Michael Collier)\textbackslash{}nSubject: Converting images to HP LaserJet III?\textbackslash{}nNntp-Posting-Host: hampton\textbackslash{}nOrganization: The City University\textbackslash{}nLines: 14\textbackslash{}n\textbackslash{}nDoes anyone know of a good way (standard PC application/PD utility) to\textbackslash{}nconvert tif/img/tga files into LaserJet III format.  We would also like to\textbackslash{}ndo the same, converting to HPGL (HP plotter) files.\textbackslash{}n\textbackslash{}nPlease email any response.\textbackslash{}n\textbackslash{}nIs this the correct group?\textbackslash{}n\textbackslash{}nThanks in advance.  Michael.\textbackslash{}n-- \textbackslash{}nMichael Collier (Programmer)                 The Computer Unit,\textbackslash{}nEmail: M.P.Collier@uk.ac.city                The City University,\textbackslash{}nTel: 071 477-8000 x3769                      London,\textbackslash{}nFax: 071 477-8565                            EC1V 0HB.\textbackslash{}n',
         "From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\textbackslash{}nSubject: help: Splitting a trimming region along a mesh \textbackslash{}nOrganization: University Of Kentucky, Dept. of Math Sciences\textbackslash{}nLines: 28\textbackslash{}n\textbackslash{}n\textbackslash{}n\textbackslash{}n\textbackslash{}tHi,\textbackslash{}n\textbackslash{}n\textbackslash{}tI have a problem, I hope some of the 'gurus' can help me solve.\textbackslash{}n\textbackslash{}n\textbackslash{}tBackground of the problem:\textbackslash{}n\textbackslash{}tI have a rectangular mesh in the uv domain, i.e  the mesh is a \textbackslash{}n\textbackslash{}tmapping of a 3d Bezier patch into 2d. The area in this domain\textbackslash{}n\textbackslash{}twhich is inside a trimming loop had to be rendered. The trimming\textbackslash{}n\textbackslash{}tloop is a set of 2d Bezier curve segments.\textbackslash{}n\textbackslash{}tFor the sake of notation: the mesh is made up of cells.\textbackslash{}n\textbackslash{}n\textbackslash{}tMy problem is this :\textbackslash{}n\textbackslash{}tThe trimming area has to be split up into individual smaller\textbackslash{}n\textbackslash{}tcells bounded by the trimming curve segments. If a cell\textbackslash{}n\textbackslash{}tis wholly inside the area{\ldots}then it is output as a whole ,\textbackslash{}n\textbackslash{}telse it is trivially rejected. \textbackslash{}n\textbackslash{}n\textbackslash{}tDoes any body know how thiss can be done, or is there any algo. \textbackslash{}n\textbackslash{}tsomewhere for doing this.\textbackslash{}n\textbackslash{}n\textbackslash{}tAny help would be appreciated.\textbackslash{}n\textbackslash{}n\textbackslash{}tThanks, \textbackslash{}n\textbackslash{}tAni.\textbackslash{}n-- \textbackslash{}nTo get irritated is human, to stay cool, divine.\textbackslash{}n"]
\end{Verbatim}
            
    \textbf{Note} the \texttt{twenty\_train} is just a bunch of objects that
can be accessed as python dictionaries; so, you can do the following
operations on \texttt{twenty\_train}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n}{twenty\PYZus{}train}\PY{o}{.}\PY{n}{target\PYZus{}names}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}5}]:} ['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n+nb}{len}\PY{p}{(}\PY{n}{twenty\PYZus{}train}\PY{o}{.}\PY{n}{data}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}6}]:} 2257
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n+nb}{len}\PY{p}{(}\PY{n}{twenty\PYZus{}train}\PY{o}{.}\PY{n}{filenames}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}7}]:} 2257
\end{Verbatim}
            
    \hypertarget{we-can-also-print-an-example-from-the-subset}{%
\paragraph{We can also print an example from the
subset}\label{we-can-also-print-an-example-from-the-subset}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{c+c1}{\PYZsh{} An example of what the subset contains}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{twenty\PYZus{}train}\PY{o}{.}\PY{n}{data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
From: sd345@city.ac.uk (Michael Collier)
Subject: Converting images to HP LaserJet III?
Nntp-Posting-Host: hampton
Organization: The City University
Lines: 14

Does anyone know of a good way (standard PC application/PD utility) to
convert tif/img/tga files into LaserJet III format.  We would also like to
do the same, converting to HPGL (HP plotter) files.

Please email any response.

Is this the correct group?

Thanks in advance.  Michael.
-- 
Michael Collier (Programmer)                 The Computer Unit,
Email: M.P.Collier@uk.ac.city                The City University,
Tel: 071 477-8000 x3769                      London,
Fax: 071 477-8565                            EC1V 0HB.


    \end{Verbatim}

    \ldots{} and determine the label of the example via
\texttt{target\_names} key value

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{twenty\PYZus{}train}\PY{o}{.}\PY{n}{target\PYZus{}names}\PY{p}{[}\PY{n}{twenty\PYZus{}train}\PY{o}{.}\PY{n}{target}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
comp.graphics

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{n}{twenty\PYZus{}train}\PY{o}{.}\PY{n}{target}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}10}]:} 1
\end{Verbatim}
            
    \ldots{} we can also get the category of 10 documents via
\texttt{target} key value

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{c+c1}{\PYZsh{} category of first 10 documents.}
        \PY{n}{twenty\PYZus{}train}\PY{o}{.}\PY{n}{target}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}6}]:} array([1, 1, 3, 3, 3, 3, 3, 2, 2, 2])
\end{Verbatim}
            
    \textbf{Note:} As you can observe, both approaches above provide two
different ways of obtaining the \texttt{category} value for the dataset.
Ideally, we want to have access to both types -- numerical and nominal
-- in the event some particular library favors a particular type.

As you may have already noticed as well, there is no \textbf{tabular
format} for the current version of the data. As data miners, we are
interested in having our dataset in the most convenient format as
possible; something we can manipulate easily and is compatible with our
algorithms, and so forth.

    Here is one way to get access to the \emph{text} version of the label of
a subset of our training data:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{k}{for} \PY{n}{t} \PY{o+ow}{in} \PY{n}{twenty\PYZus{}train}\PY{o}{.}\PY{n}{target}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{n}{twenty\PYZus{}train}\PY{o}{.}\PY{n}{target\PYZus{}names}\PY{p}{[}\PY{n}{t}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
comp.graphics
comp.graphics
soc.religion.christian
soc.religion.christian
soc.religion.christian
soc.religion.christian
soc.religion.christian
sci.med
sci.med
sci.med

    \end{Verbatim}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{exercise-1-5-min-in-this-exercise-please-print-out-the-text-data-for-the-first-three-samples-in-the-dataset.-see-the-above-code-for-help}{%
\subsubsection{\texorpdfstring{** \textgreater\textgreater\textgreater{}
Exercise 1 (5 min): **\\
In this exercise, please print out the \emph{text} data for the first
three samples in the dataset. (See the above code for
help)}{** \textgreater\textgreater\textgreater{} Exercise 1 (5 min): ** In this exercise, please print out the text data for the first three samples in the dataset. (See the above code for help)}}\label{exercise-1-5-min-in-this-exercise-please-print-out-the-text-data-for-the-first-three-samples-in-the-dataset.-see-the-above-code-for-help}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{c+c1}{\PYZsh{} Answer here}
        \PY{k}{for} \PY{n}{t} \PY{o+ow}{in} \PY{n}{twenty\PYZus{}train}\PY{o}{.}\PY{n}{data}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{n}{t}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
From: sd345@city.ac.uk (Michael Collier)
Subject: Converting images to HP LaserJet III?
Nntp-Posting-Host: hampton
Organization: The City University
Lines: 14

Does anyone know of a good way (standard PC application/PD utility) to
convert tif/img/tga files into LaserJet III format.  We would also like to
do the same, converting to HPGL (HP plotter) files.

Please email any response.

Is this the correct group?

Thanks in advance.  Michael.
-- 
Michael Collier (Programmer)                 The Computer Unit,
Email: M.P.Collier@uk.ac.city                The City University,
Tel: 071 477-8000 x3769                      London,
Fax: 071 477-8565                            EC1V 0HB.

From: ani@ms.uky.edu (Aniruddha B. Deglurkar)
Subject: help: Splitting a trimming region along a mesh 
Organization: University Of Kentucky, Dept. of Math Sciences
Lines: 28



	Hi,

	I have a problem, I hope some of the 'gurus' can help me solve.

	Background of the problem:
	I have a rectangular mesh in the uv domain, i.e  the mesh is a 
	mapping of a 3d Bezier patch into 2d. The area in this domain
	which is inside a trimming loop had to be rendered. The trimming
	loop is a set of 2d Bezier curve segments.
	For the sake of notation: the mesh is made up of cells.

	My problem is this :
	The trimming area has to be split up into individual smaller
	cells bounded by the trimming curve segments. If a cell
	is wholly inside the area{\ldots}then it is output as a whole ,
	else it is trivially rejected. 

	Does any body know how thiss can be done, or is there any algo. 
	somewhere for doing this.

	Any help would be appreciated.

	Thanks, 
	Ani.
-- 
To get irritated is human, to stay cool, divine.

From: djohnson@cs.ucsd.edu (Darin Johnson)
Subject: Re: harrassed at work, could use some prayers
Organization: =CSE Dept., U.C. San Diego
Lines: 63

(Well, I'll email also, but this may apply to other people, so
I'll post also.)

>I've been working at this company for eight years in various
>engineering jobs.  I'm female.  Yesterday I counted and realized that
>on seven different occasions I've been sexually harrassed at this
>company.

>I dreaded coming back to work today.  What if my boss comes in to ask
>me some kind of question{\ldots}

Your boss should be the person bring these problems to.  If he/she
does not seem to take any action, keep going up higher and higher.
Sexual harrassment does not need to be tolerated, and it can be an
enormous emotional support to discuss this with someone and know that
they are trying to do something about it.  If you feel you can not
discuss this with your boss, perhaps your company has a personnel
department that can work for you while preserving your privacy.  Most
companies will want to deal with this problem because constant anxiety
does seriously affect how effectively employees do their jobs.

It is unclear from your letter if you have done this or not.  It is
not inconceivable that management remains ignorant of employee
problems/strife even after eight years (it's a miracle if they do
notice).  Perhaps your manager did not bring to the attention of
higher ups?  If the company indeed does seem to want to ignore the
entire problem, there may be a state agency willing to fight with
you.  (check with a lawyer, a women's resource center, etc to find out)

You may also want to discuss this with your paster, priest, husband,
etc.  That is, someone you know will not be judgemental and that is
supportive, comforting, etc.  This will bring a lot of healing.

>So I returned at 11:25, only to find that ever single
>person had already left for lunch.  They left at 11:15 or so.  No one
>could be bothered to call me at the other building, even though my
>number was posted.

This happens to a lot of people.  Honest.  I believe it may seem
to be due to gross insensitivity because of the feelings you are
going through.  People in offices tend to be more insensitive while
working than they normally are (maybe it's the hustle or stress or{\ldots})
I've had this happen to me a lot, often because they didn't realize
my car was broken, etc.  Then they will come back and wonder why I
didn't want to go (this would tend to make me stop being angry at
being ignored and make me laugh).  Once, we went off without our
boss, who was paying for the lunch :-)

>For this
>reason I hope good Mr. Moderator allows me this latest indulgence.

Well, if you can't turn to the computer for support, what would
we do?  (signs of the computer age :-)

In closing, please don't let the hateful actions of a single person
harm you.  They are doing it because they are still the playground
bully and enjoy seeing the hurt they cause.  And you should not
accept the opinions of an imbecile that you are worthless - much
wiser people hold you in great esteem.
-- 
Darin Johnson
djohnson@ucsd.edu
  - Luxury!  In MY day, we had to make do with 5 bytes of swap{\ldots}


    \end{Verbatim}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{data-transformation}{%
\subsection{3. Data Transformation}\label{data-transformation}}

So we want to explore and understand our data a little bit better.
Before we do that we definitely need to apply some transformations just
so we can have our dataset in a nice format to be able to explore it
freely and more efficient. Lucky for us, there are powerful scientific
tools to transform our data into that tabular format we are so farmiliar
with. So that is what we will do in the next section--transform our data
into a nice table format.

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{converting-dictionary-into-pandas-dataframe}{%
\subsubsection{3.1 Converting Dictionary into Pandas
Dataframe}\label{converting-dictionary-into-pandas-dataframe}}

Here we will show you how to convert dictionary objects into a pandas
dataframe. And by the way, a pandas dataframe is nothing more than a
table magically stored for efficient information retrieval.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} \PY{n}{twenty\PYZus{}train}\PY{o}{.}\PY{n}{data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}36}]:} ['From: sd345@city.ac.uk (Michael Collier)\textbackslash{}nSubject: Converting images to HP LaserJet III?\textbackslash{}nNntp-Posting-Host: hampton\textbackslash{}nOrganization: The City University\textbackslash{}nLines: 14\textbackslash{}n\textbackslash{}nDoes anyone know of a good way (standard PC application/PD utility) to\textbackslash{}nconvert tif/img/tga files into LaserJet III format.  We would also like to\textbackslash{}ndo the same, converting to HPGL (HP plotter) files.\textbackslash{}n\textbackslash{}nPlease email any response.\textbackslash{}n\textbackslash{}nIs this the correct group?\textbackslash{}n\textbackslash{}nThanks in advance.  Michael.\textbackslash{}n-- \textbackslash{}nMichael Collier (Programmer)                 The Computer Unit,\textbackslash{}nEmail: M.P.Collier@uk.ac.city                The City University,\textbackslash{}nTel: 071 477-8000 x3769                      London,\textbackslash{}nFax: 071 477-8565                            EC1V 0HB.\textbackslash{}n',
          "From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\textbackslash{}nSubject: help: Splitting a trimming region along a mesh \textbackslash{}nOrganization: University Of Kentucky, Dept. of Math Sciences\textbackslash{}nLines: 28\textbackslash{}n\textbackslash{}n\textbackslash{}n\textbackslash{}n\textbackslash{}tHi,\textbackslash{}n\textbackslash{}n\textbackslash{}tI have a problem, I hope some of the 'gurus' can help me solve.\textbackslash{}n\textbackslash{}n\textbackslash{}tBackground of the problem:\textbackslash{}n\textbackslash{}tI have a rectangular mesh in the uv domain, i.e  the mesh is a \textbackslash{}n\textbackslash{}tmapping of a 3d Bezier patch into 2d. The area in this domain\textbackslash{}n\textbackslash{}twhich is inside a trimming loop had to be rendered. The trimming\textbackslash{}n\textbackslash{}tloop is a set of 2d Bezier curve segments.\textbackslash{}n\textbackslash{}tFor the sake of notation: the mesh is made up of cells.\textbackslash{}n\textbackslash{}n\textbackslash{}tMy problem is this :\textbackslash{}n\textbackslash{}tThe trimming area has to be split up into individual smaller\textbackslash{}n\textbackslash{}tcells bounded by the trimming curve segments. If a cell\textbackslash{}n\textbackslash{}tis wholly inside the area{\ldots}then it is output as a whole ,\textbackslash{}n\textbackslash{}telse it is trivially rejected. \textbackslash{}n\textbackslash{}n\textbackslash{}tDoes any body know how thiss can be done, or is there any algo. \textbackslash{}n\textbackslash{}tsomewhere for doing this.\textbackslash{}n\textbackslash{}n\textbackslash{}tAny help would be appreciated.\textbackslash{}n\textbackslash{}n\textbackslash{}tThanks, \textbackslash{}n\textbackslash{}tAni.\textbackslash{}n-- \textbackslash{}nTo get irritated is human, to stay cool, divine.\textbackslash{}n"]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{n}{twenty\PYZus{}train}\PY{o}{.}\PY{n}{target}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}37}]:} array([1, 1, 3, {\ldots}, 2, 2, 2])
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd} \PY{c+c1}{\PYZsh{} manipulate objects define pandas}
         
         \PY{c+c1}{\PYZsh{} my functions}
         \PY{k+kn}{import} \PY{n+nn}{helpers}\PY{n+nn}{.}\PY{n+nn}{data\PYZus{}mining\PYZus{}helpers} \PY{k}{as} \PY{n+nn}{dmh} 
         \PY{c+c1}{\PYZsh{} folder called helper it has some function implementated}
         \PY{c+c1}{\PYZsh{} dmh data mining helper}
         \PY{c+c1}{\PYZsh{} doc1,doc2,doc3}
         \PY{c+c1}{\PYZsh{} construct dataframe from a list}
         \PY{c+c1}{\PYZsh{} create dataframe which will take 20 format into rows }
         \PY{c+c1}{\PYZsh{} become text column}
         \PY{c+c1}{\PYZsh{} doc1}
         \PY{c+c1}{\PYZsh{} doc2}
         \PY{c+c1}{\PYZsh{} doc3}
         \PY{n}{X} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{o}{.}\PY{n}{from\PYZus{}records}\PY{p}{(}\PY{n}{dmh}\PY{o}{.}\PY{n}{format\PYZus{}rows}\PY{p}{(}\PY{n}{twenty\PYZus{}train}\PY{p}{)}\PY{p}{,} \PY{n}{columns}\PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}42}]:} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} check we did not lose any data}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}42}]:} 2257
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{n}{X}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}40}]:}                                                 text
         0  From: sd345@city.ac.uk (Michael Collier) Subje{\ldots}
         1  From: ani@ms.uky.edu (Aniruddha B. Deglurkar) {\ldots}
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{k}{for} \PY{n}{t} \PY{o+ow}{in} \PY{n}{X}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{text}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{:}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{t}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} diff way to print from data structure}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
From: sd345@city.ac.uk (Michael Collier) Subject: Converting images to HP LaserJet III? Nntp-Posting-Host: hampton Organization: The City University Lines: 14  Does anyone know of a good way (standard PC application/PD utility) to convert tif/img/tga files into LaserJet III format.  We would also like to do the same, converting to HPGL (HP plotter) files.  Please email any response.  Is this the correct group?  Thanks in advance.  Michael. --  Michael Collier (Programmer)                 The Computer Unit, Email: M.P.Collier@uk.ac.city                The City University, Tel: 071 477-8000 x3769                      London, Fax: 071 477-8565                            EC1V 0HB. 
From: ani@ms.uky.edu (Aniruddha B. Deglurkar) Subject: help: Splitting a trimming region along a mesh  Organization: University Of Kentucky, Dept. of Math Sciences Lines: 28    	Hi,  	I have a problem, I hope some of the 'gurus' can help me solve.  	Background of the problem: 	I have a rectangular mesh in the uv domain, i.e  the mesh is a  	mapping of a 3d Bezier patch into 2d. The area in this domain 	which is inside a trimming loop had to be rendered. The trimming 	loop is a set of 2d Bezier curve segments. 	For the sake of notation: the mesh is made up of cells.  	My problem is this : 	The trimming area has to be split up into individual smaller 	cells bounded by the trimming curve segments. If a cell 	is wholly inside the area{\ldots}then it is output as a whole , 	else it is trivially rejected.   	Does any body know how thiss can be done, or is there any algo.  	somewhere for doing this.  	Any help would be appreciated.  	Thanks,  	Ani. --  To get irritated is human, to stay cool, divine. 
From: djohnson@cs.ucsd.edu (Darin Johnson) Subject: Re: harrassed at work, could use some prayers Organization: =CSE Dept., U.C. San Diego Lines: 63  (Well, I'll email also, but this may apply to other people, so I'll post also.)  >I've been working at this company for eight years in various >engineering jobs.  I'm female.  Yesterday I counted and realized that >on seven different occasions I've been sexually harrassed at this >company.  >I dreaded coming back to work today.  What if my boss comes in to ask >me some kind of question{\ldots}  Your boss should be the person bring these problems to.  If he/she does not seem to take any action, keep going up higher and higher. Sexual harrassment does not need to be tolerated, and it can be an enormous emotional support to discuss this with someone and know that they are trying to do something about it.  If you feel you can not discuss this with your boss, perhaps your company has a personnel department that can work for you while preserving your privacy.  Most companies will want to deal with this problem because constant anxiety does seriously affect how effectively employees do their jobs.  It is unclear from your letter if you have done this or not.  It is not inconceivable that management remains ignorant of employee problems/strife even after eight years (it's a miracle if they do notice).  Perhaps your manager did not bring to the attention of higher ups?  If the company indeed does seem to want to ignore the entire problem, there may be a state agency willing to fight with you.  (check with a lawyer, a women's resource center, etc to find out)  You may also want to discuss this with your paster, priest, husband, etc.  That is, someone you know will not be judgemental and that is supportive, comforting, etc.  This will bring a lot of healing.  >So I returned at 11:25, only to find that ever single >person had already left for lunch.  They left at 11:15 or so.  No one >could be bothered to call me at the other building, even though my >number was posted.  This happens to a lot of people.  Honest.  I believe it may seem to be due to gross insensitivity because of the feelings you are going through.  People in offices tend to be more insensitive while working than they normally are (maybe it's the hustle or stress or{\ldots}) I've had this happen to me a lot, often because they didn't realize my car was broken, etc.  Then they will come back and wonder why I didn't want to go (this would tend to make me stop being angry at being ignored and make me laugh).  Once, we went off without our boss, who was paying for the lunch :-)  >For this >reason I hope good Mr. Moderator allows me this latest indulgence.  Well, if you can't turn to the computer for support, what would we do?  (signs of the computer age :-)  In closing, please don't let the hateful actions of a single person harm you.  They are doing it because they are still the playground bully and enjoy seeing the hurt they cause.  And you should not accept the opinions of an imbecile that you are worthless - much wiser people hold you in great esteem. --  Darin Johnson djohnson@ucsd.edu   - Luxury!  In MY day, we had to make do with 5 bytes of swap{\ldots} 

    \end{Verbatim}

    \hypertarget{adding-columns}{%
\subsubsection{Adding Columns}\label{adding-columns}}

    One of the great advantages of a pandas dataframe is its flexibility. We
can add columns to the current dataset programmatically with very little
effort.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}43}]:} \PY{c+c1}{\PYZsh{} add category to the dataframe}
         \PY{c+c1}{\PYZsh{} x include categoty  if it no column it means create it}
         \PY{n}{X}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{category}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{twenty\PYZus{}train}\PY{o}{.}\PY{n}{target}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}44}]:} \PY{c+c1}{\PYZsh{} add category label also}
         \PY{n}{X}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{category\PYZus{}name}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{category}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{t}\PY{p}{:} \PY{n}{dmh}\PY{o}{.}\PY{n}{format\PYZus{}labels}\PY{p}{(}\PY{n}{t}\PY{p}{,} \PY{n}{twenty\PYZus{}train}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    Now we can print and see what our table looks like.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{n}{X}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}22}]:}                                                 text  category  \textbackslash{}
         0  From: sd345@city.ac.uk (Michael Collier) Subje{\ldots}         1   
         1  From: ani@ms.uky.edu (Aniruddha B. Deglurkar) {\ldots}         1   
         2  From: djohnson@cs.ucsd.edu (Darin Johnson) Sub{\ldots}         3   
         3  From: s0612596@let.rug.nl (M.M. Zwart) Subject{\ldots}         3   
         4  From: stanly@grok11.columbiasc.ncr.com (stanly{\ldots}         3   
         5  From: vbv@lor.eeap.cwru.edu (Virgilio (Dean) B{\ldots}         3   
         6  From: jodfishe@silver.ucs.indiana.edu (joseph {\ldots}         3   
         7  From: aldridge@netcom.com (Jacquelin Aldridge){\ldots}         2   
         8  From: geb@cs.pitt.edu (Gordon Banks) Subject: {\ldots}         2   
         9  From: libman@hsc.usc.edu (Marlena Libman) Subj{\ldots}         2   
         
                     category\_name  
         0           comp.graphics  
         1           comp.graphics  
         2  soc.religion.christian  
         3  soc.religion.christian  
         4  soc.religion.christian  
         5  soc.religion.christian  
         6  soc.religion.christian  
         7                 sci.med  
         8                 sci.med  
         9                 sci.med  
\end{Verbatim}
            
    Nice! Isn't it? With this format we can conduct many operations easily
and efficiently since Pandas dataframes provide us with a wide range of
built-in features/functionalities. These features are operations which
can directly and quickly be applied to the dataset. These operations may
include standard operations like \textbf{removing records with missing
values} and \textbf{aggregating new fields} to the current table
(hereinafter referred to as a dataframe), which is desirable in almost
every data mining project. Go Pandas!

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{familiarizing-yourself-with-the-data}{%
\subsubsection{3.2 Familiarizing yourself with the
Data}\label{familiarizing-yourself-with-the-data}}

    To begin to show you the awesomeness of Pandas dataframes, let us look
at how to run a simple query on our dataset. We want to query for the
first 10 rows (documents), and we only want to keep the \texttt{text}
and \texttt{category\_name} attributes or fields.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{c+c1}{\PYZsh{} a simple query}
         \PY{n}{X}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}\PY{p}{[}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{text}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{category\PYZus{}name}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}23}]:}                                                 text           category\_name
         0  From: sd345@city.ac.uk (Michael Collier) Subje{\ldots}           comp.graphics
         1  From: ani@ms.uky.edu (Aniruddha B. Deglurkar) {\ldots}           comp.graphics
         2  From: djohnson@cs.ucsd.edu (Darin Johnson) Sub{\ldots}  soc.religion.christian
         3  From: s0612596@let.rug.nl (M.M. Zwart) Subject{\ldots}  soc.religion.christian
         4  From: stanly@grok11.columbiasc.ncr.com (stanly{\ldots}  soc.religion.christian
         5  From: vbv@lor.eeap.cwru.edu (Virgilio (Dean) B{\ldots}  soc.religion.christian
         6  From: jodfishe@silver.ucs.indiana.edu (joseph {\ldots}  soc.religion.christian
         7  From: aldridge@netcom.com (Jacquelin Aldridge){\ldots}                 sci.med
         8  From: geb@cs.pitt.edu (Gordon Banks) Subject: {\ldots}                 sci.med
         9  From: libman@hsc.usc.edu (Marlena Libman) Subj{\ldots}                 sci.med
\end{Verbatim}
            
    Let us look at a few more interesting queries to familiarize ourselves
with the efficiency and conveniency of Pandas dataframes.

    \hypertarget{lets-query-the-last-10-records}{%
\paragraph{Let's query the last 10
records}\label{lets-query-the-last-10-records}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{n}{X}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{10}\PY{p}{:}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}24}]:}                                                    text  category  \textbackslash{}
         2247  From: daniels@math.ufl.edu (TV's Big Dealer) S{\ldots}         3   
         2248  From: "danny hawrysio" <danny.hawrysio@canrem{\ldots}         1   
         2249  From: shellgate!llo@uu4.psi.com (Larry L. Over{\ldots}         3   
         2250  From: ingles@engin.umich.edu (Ray Ingles) Subj{\ldots}         0   
         2251  From: Mark-Tarbell@suite.com Subject: Amniocen{\ldots}         2   
         2252  From: roos@Operoni.Helsinki.FI (Christophe Roo{\ldots}         2   
         2253  From: mhollowa@ic.sunysb.edu (Michael Holloway{\ldots}         2   
         2254  From: sasghm@theseus.unx.sas.com (Gary Merrill{\ldots}         2   
         2255  From: Dan Wallach <dwallach@cs.berkeley.edu> S{\ldots}         2   
         2256  From: dyer@spdcc.com (Steve Dyer) Subject: Re:{\ldots}         2   
         
                        category\_name  
         2247  soc.religion.christian  
         2248           comp.graphics  
         2249  soc.religion.christian  
         2250             alt.atheism  
         2251                 sci.med  
         2252                 sci.med  
         2253                 sci.med  
         2254                 sci.med  
         2255                 sci.med  
         2256                 sci.med  
\end{Verbatim}
            
    Ready for some sourcery? Brace yourselves! Let us see if we can query
every 10th record in our dataframe. In addition, our query must only
contain the first 10 records. For this we will use the build-in function
called \texttt{iloc}. This allows us to query a selection of our dataset
by position.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}45}]:} \PY{c+c1}{\PYZsh{} using loc (by position)}
         \PY{n}{X}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]} 
         
         \PY{c+c1}{\PYZsh{} every 10, first 2 column , limit the amount want to see (10 results be displayed)}
         \PY{c+c1}{\PYZsh{} query specific things by function}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}45}]:}                                                  text  category
         0   From: sd345@city.ac.uk (Michael Collier) Subje{\ldots}         1
         10  From: anasaz!karl@anasazi.com (Karl Dussik) Su{\ldots}         3
         20  From: dotsonm@dmapub.dma.org (Mark Dotson) Sub{\ldots}         3
         30  From: vgwlu@dunsell.calgary.chevron.com (greg {\ldots}         2
         40  From: david-s@hsr.no (David A. Sjoen) Subject:{\ldots}         3
         50  From: ab@nova.cc.purdue.edu (Allen B) Subject:{\ldots}         1
         60  From: Nanci Ann Miller <nm0w+@andrew.cmu.edu> {\ldots}         0
         70  From: weaver@chdasic.sps.mot.com (Dave Weaver){\ldots}         3
         80  From: annick@cortex.physiol.su.oz.au (Annick A{\ldots}         2
         90  Subject: Vonnegut/atheism From: dmn@kepler.unh{\ldots}         0
\end{Verbatim}
            
    You can also use the \texttt{loc} function to explicity define the
columns you want to query. Take a look at this
\href{https://stackoverflow.com/questions/28757389/pandas-loc-vs-iloc-vs-ix-vs-at-vs-iat/43968774}{great
discussion} on the differences between the \texttt{iloc} and
\texttt{loc} functions.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}46}]:} \PY{c+c1}{\PYZsh{} using loc (by label)}
         \PY{n}{X}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}46}]:} 0     From: sd345@city.ac.uk (Michael Collier) Subje{\ldots}
         10    From: anasaz!karl@anasazi.com (Karl Dussik) Su{\ldots}
         20    From: dotsonm@dmapub.dma.org (Mark Dotson) Sub{\ldots}
         30    From: vgwlu@dunsell.calgary.chevron.com (greg {\ldots}
         40    From: david-s@hsr.no (David A. Sjoen) Subject:{\ldots}
         50    From: ab@nova.cc.purdue.edu (Allen B) Subject:{\ldots}
         60    From: Nanci Ann Miller <nm0w+@andrew.cmu.edu> {\ldots}
         70    From: weaver@chdasic.sps.mot.com (Dave Weaver){\ldots}
         80    From: annick@cortex.physiol.su.oz.au (Annick A{\ldots}
         90    Subject: Vonnegut/atheism From: dmn@kepler.unh{\ldots}
         Name: text, dtype: object
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{c+c1}{\PYZsh{} standard query (Cannot simultaneously select rows and columns)}
         \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}27}]:}                                                  text  category  \textbackslash{}
         0   From: sd345@city.ac.uk (Michael Collier) Subje{\ldots}         1   
         10  From: anasaz!karl@anasazi.com (Karl Dussik) Su{\ldots}         3   
         20  From: dotsonm@dmapub.dma.org (Mark Dotson) Sub{\ldots}         3   
         30  From: vgwlu@dunsell.calgary.chevron.com (greg {\ldots}         2   
         40  From: david-s@hsr.no (David A. Sjoen) Subject:{\ldots}         3   
         50  From: ab@nova.cc.purdue.edu (Allen B) Subject:{\ldots}         1   
         60  From: Nanci Ann Miller <nm0w+@andrew.cmu.edu> {\ldots}         0   
         70  From: weaver@chdasic.sps.mot.com (Dave Weaver){\ldots}         3   
         80  From: annick@cortex.physiol.su.oz.au (Annick A{\ldots}         2   
         90  Subject: Vonnegut/atheism From: dmn@kepler.unh{\ldots}         0   
         
                      category\_name  
         0            comp.graphics  
         10  soc.religion.christian  
         20  soc.religion.christian  
         30                 sci.med  
         40  soc.religion.christian  
         50           comp.graphics  
         60             alt.atheism  
         70  soc.religion.christian  
         80                 sci.med  
         90             alt.atheism  
\end{Verbatim}
            
    \hypertarget{exercise-2-take-home}{%
\subsubsection{** \textgreater\textgreater\textgreater{} Exercise 2
(take home):**}\label{exercise-2-take-home}}

Experiment with other querying techniques using pandas dataframes. Refer
to their
\href{https://pandas.pydata.org/pandas-docs/stable/indexing.html}{documentation}
for more information.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{c+c1}{\PYZsh{}Answer here}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{exercise-3-5-min-try-to-fecth-records-belonging-to-the-comp.graphics-category-and-query-every-10th-record.-only-show-the-first-5-records.}{%
\subsubsection{\texorpdfstring{** \textgreater\textgreater\textgreater{}
Exercise 3 (5 min): **\\
Try to fecth records belonging to the \texttt{comp.graphics} category,
and query every 10th record. Only show the first 5
records.}{** \textgreater\textgreater\textgreater{} Exercise 3 (5 min): ** Try to fecth records belonging to the comp.graphics category, and query every 10th record. Only show the first 5 records.}}\label{exercise-3-5-min-try-to-fecth-records-belonging-to-the-comp.graphics-category-and-query-every-10th-record.-only-show-the-first-5-records.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}47}]:} \PY{c+c1}{\PYZsh{} Answer here}
         \PY{c+c1}{\PYZsh{} X[X[\PYZsq{}category\PYZus{}name\PYZsq{}]==sci.med].iloc[::10][0:5]}
         \PY{n}{X}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{k}{lambda} \PY{n}{f}\PY{p}{:} \PY{n}{f}\PY{o}{.}\PY{n}{category\PYZus{}name} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{comp.graphics}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}47}]:}                                                   text  category  \textbackslash{}
         0    From: sd345@city.ac.uk (Michael Collier) Subje{\ldots}         1   
         43   From: zyeh@caspian.usc.edu (zhenghao yeh) Subj{\ldots}         1   
         76   From: sts@mfltd.co.uk (Steve Sherwood (x5543)){\ldots}         1   
         107  From: samson@prlhp1.prl.philips.co.uk (Mark Sa{\ldots}         1   
         172  From: thinman@netcom.com (Technically Sweet) S{\ldots}         1   
         
              category\_name  
         0    comp.graphics  
         43   comp.graphics  
         76   comp.graphics  
         107  comp.graphics  
         172  comp.graphics  
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{o}{\PYZhy{}}\PY{o}{\PYZhy{}}\PY{o}{\PYZhy{}}
\end{Verbatim}


    \hypertarget{data-mining-using-pandas}{%
\subsection{4. Data Mining using
Pandas}\label{data-mining-using-pandas}}

    Let's do some serious work now. Let's learn to program some of the ideas
and concepts learned so far in the data mining course. This is the only
way we can be convince ourselves of the true power of Pandas dataframes.

    \hypertarget{missing-values}{%
\subsubsection{4.1 Missing Values}\label{missing-values}}

    First, let us consider that our dataset has some \emph{missing values}
and we want to remove those values. In its current state our dataset has
no missing values, but for practice sake we will add some records with
missing values and then write some code to deal with these objects that
contain missing values. You will see for yourself how easy it is to deal
with missing values once you have your data transformed into a Pandas
dataframe.

Before we jump into coding, let us do a quick review of what we have
learned in the Data Mining course. Specifically, let's review the
methods used to deal with missing values.

The most common reasons for having missing values in datasets has to do
with how the data was initially collected. A good example of this is
when a patient comes into the ER room, the data is collected as quickly
as possible and depending on the conditions of the patients, the
personal data being collected is either incomplete or partially
complete. In the former and latter cases, we are presented with a case
of ``missing values''. Knowing that patients data is particularly
critical and can be used by the health authorities to conduct some
interesting analysis, we as the data miners are left with the tough task
of deciding what to do with these missing and incomplete records. We
need to deal with these records because they are definitely going to
affect our analysis or learning algorithms. So what do we do? There are
several ways to handle missing values, and some of the more effective
ways are presented below (Note: You can reference the slides - Session 1
Handout for the additional information).

\begin{itemize}
\item
  \textbf{Eliminate Data Objects} - Here we completely discard records
  once they contain some missing values. This is the easiest approach
  and the one we will be using in this notebook. The immediate drawback
  of going with this approach is that you lose some information, and in
  some cases too much of it. Now imagine that half of the records have
  at least one or more missing values. Here you are presented with the
  tough decision of quantity vs quality. In any event, this decision
  must be made carefully, hence the reason for emphasizing it here in
  this notebook.
\item
  \textbf{Estimate Missing Values} - Here we try to estimate the missing
  values based on some criteria. Although this approach may be proven to
  be effective, it is not always the case, especially when we are
  dealing with sensitive data, like \textbf{Gender} or \textbf{Names}.
  For fields like \textbf{Address}, there could be ways to obtain these
  missing addresses using some data aggregation technique or obtain the
  information directly from other databases or public data sources.
\item
  \textbf{Ignore the missing value during analysis} - Here we basically
  ignore the missing values and proceed with our analysis. Although this
  is the most naive way to handle missing values it may proof effective,
  especially when the missing values includes information that is not
  important to the analysis being conducted. But think about it for a
  while. Would you ignore missing values, especially when in this day
  and age it is difficult to obtain high quality datasets? Again, there
  are some tradeoffs, which we will talk about later in the notebook.
\item
  \textbf{Replace with all possible values} - As an efficient and
  responsible data miner, we sometimes just need to put in the hard
  hours of work and find ways to makes up for these missing values. This
  last option is a very wise option for cases where data is scarce
  (which is almost always) or when dealing with sensitive data. Imagine
  that our dataset has an \textbf{Age} field, which contains many
  missing values. Since \textbf{Age} is a continuous variable, it means
  that we can build a separate model for calculating the age for the
  incomplete records based on some rule-based appraoch or probabilistic
  approach.
\end{itemize}

    As mentioned earlier, we are going to go with the first option but you
may be asked to compute missing values, using a different approach, as
an exercise. Let's get to it!

First we want to add the dummy records with missing values since the
dataset we have is perfectly composed and cleaned that it contains no
missing values. First let us check for ourselves that indeed the dataset
doesn't contain any missing values. We can do that easily by using the
following built-in function provided by Pandas.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}48}]:} \PY{n}{X}\PY{o}{.}\PY{n}{isnull}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}48}]:}        text  category  category\_name
         0     False     False          False
         1     False     False          False
         2     False     False          False
         3     False     False          False
         4     False     False          False
         5     False     False          False
         6     False     False          False
         7     False     False          False
         8     False     False          False
         9     False     False          False
         10    False     False          False
         11    False     False          False
         12    False     False          False
         13    False     False          False
         14    False     False          False
         15    False     False          False
         16    False     False          False
         17    False     False          False
         18    False     False          False
         19    False     False          False
         20    False     False          False
         21    False     False          False
         22    False     False          False
         23    False     False          False
         24    False     False          False
         25    False     False          False
         26    False     False          False
         27    False     False          False
         28    False     False          False
         29    False     False          False
         {\ldots}     {\ldots}       {\ldots}            {\ldots}
         2227  False     False          False
         2228  False     False          False
         2229  False     False          False
         2230  False     False          False
         2231  False     False          False
         2232  False     False          False
         2233  False     False          False
         2234  False     False          False
         2235  False     False          False
         2236  False     False          False
         2237  False     False          False
         2238  False     False          False
         2239  False     False          False
         2240  False     False          False
         2241  False     False          False
         2242  False     False          False
         2243  False     False          False
         2244  False     False          False
         2245  False     False          False
         2246  False     False          False
         2247  False     False          False
         2248  False     False          False
         2249  False     False          False
         2250  False     False          False
         2251  False     False          False
         2252  False     False          False
         2253  False     False          False
         2254  False     False          False
         2255  False     False          False
         2256  False     False          False
         
         [2257 rows x 3 columns]
\end{Verbatim}
            
    The \texttt{isnull} function looks through the entire dataset for null
values and returns \texttt{True} wherever it finds any missing field or
record. As you will see above, and as we anticipated, our dataset looks
clean and all values are present, since \texttt{isnull} returns
\textbf{False} for all fields and records. But let us start to get our
hands dirty and build a nice little function to check each of the
records, column by column, and return a nice little message telling us
the amount of missing records found. This excerice will also encourage
us to explore other capabilities of pandas dataframes. In most cases,
the build-in functions are good enough, but as you saw above when the
entire table was printed, it is impossible to tell if there are missing
records just by looking at preview of records manually, especially in
cases where the dataset is huge. We want a more reliable way to achieve
this. Let's get to it!

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}49}]:} \PY{n}{X}\PY{o}{.}\PY{n}{isnull}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{dmh}\PY{o}{.}\PY{n}{check\PYZus{}missing\PYZus{}values}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}49}]:} text             (The amoung of missing records is: , 0)
         category         (The amoung of missing records is: , 0)
         category\_name    (The amoung of missing records is: , 0)
         dtype: object
\end{Verbatim}
            
    Okay, a lot happened there in that one line of code, so let's break it
down. First, with the \texttt{isnull} we tranformed our table into the
\textbf{True/False} table you see above, where \textbf{True} in this
case means that the data is missing and \textbf{False} means that the
data is present. We then take the transformed table and apply a function
to each row that essentially counts to see if there are missing values
in each record and print out how much missing values we found. In other
words the \texttt{check\_missing\_values} function looks through each
field (attribute or column) in the dataset and counts how many missing
values were found.

There are many other clever ways to check for missing data, and that is
what makes Pandas so beautiful to work with. You get the control you
need as a data scientist or just a person working in data mining
projects. Indeed, Pandas makes your life easy!

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{exercise-4-5-min}{%
\subsubsection{\texorpdfstring{\textgreater\textgreater\textgreater{}
\textbf{Exercise 4 (5
min):}}{\textgreater\textgreater\textgreater{} Exercise 4 (5 min):}}\label{exercise-4-5-min}}

Let's try something different. Instead of calculating missing values by
column let's try to calculate the missing values in every record instead
of every column.\\
\(Hint\) : \texttt{axis} parameter. Check the documentation for more
information.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}50}]:} \PY{c+c1}{\PYZsh{} Answer here}
         \PY{n}{X}\PY{o}{.}\PY{n}{isnull}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{dmh}\PY{o}{.}\PY{n}{check\PYZus{}missing\PYZus{}values}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}50}]:} 0       (The amoung of missing records is: , 0)
         1       (The amoung of missing records is: , 0)
         2       (The amoung of missing records is: , 0)
         3       (The amoung of missing records is: , 0)
         4       (The amoung of missing records is: , 0)
         5       (The amoung of missing records is: , 0)
         6       (The amoung of missing records is: , 0)
         7       (The amoung of missing records is: , 0)
         8       (The amoung of missing records is: , 0)
         9       (The amoung of missing records is: , 0)
         10      (The amoung of missing records is: , 0)
         11      (The amoung of missing records is: , 0)
         12      (The amoung of missing records is: , 0)
         13      (The amoung of missing records is: , 0)
         14      (The amoung of missing records is: , 0)
         15      (The amoung of missing records is: , 0)
         16      (The amoung of missing records is: , 0)
         17      (The amoung of missing records is: , 0)
         18      (The amoung of missing records is: , 0)
         19      (The amoung of missing records is: , 0)
         20      (The amoung of missing records is: , 0)
         21      (The amoung of missing records is: , 0)
         22      (The amoung of missing records is: , 0)
         23      (The amoung of missing records is: , 0)
         24      (The amoung of missing records is: , 0)
         25      (The amoung of missing records is: , 0)
         26      (The amoung of missing records is: , 0)
         27      (The amoung of missing records is: , 0)
         28      (The amoung of missing records is: , 0)
         29      (The amoung of missing records is: , 0)
                                  {\ldots}                   
         2227    (The amoung of missing records is: , 0)
         2228    (The amoung of missing records is: , 0)
         2229    (The amoung of missing records is: , 0)
         2230    (The amoung of missing records is: , 0)
         2231    (The amoung of missing records is: , 0)
         2232    (The amoung of missing records is: , 0)
         2233    (The amoung of missing records is: , 0)
         2234    (The amoung of missing records is: , 0)
         2235    (The amoung of missing records is: , 0)
         2236    (The amoung of missing records is: , 0)
         2237    (The amoung of missing records is: , 0)
         2238    (The amoung of missing records is: , 0)
         2239    (The amoung of missing records is: , 0)
         2240    (The amoung of missing records is: , 0)
         2241    (The amoung of missing records is: , 0)
         2242    (The amoung of missing records is: , 0)
         2243    (The amoung of missing records is: , 0)
         2244    (The amoung of missing records is: , 0)
         2245    (The amoung of missing records is: , 0)
         2246    (The amoung of missing records is: , 0)
         2247    (The amoung of missing records is: , 0)
         2248    (The amoung of missing records is: , 0)
         2249    (The amoung of missing records is: , 0)
         2250    (The amoung of missing records is: , 0)
         2251    (The amoung of missing records is: , 0)
         2252    (The amoung of missing records is: , 0)
         2253    (The amoung of missing records is: , 0)
         2254    (The amoung of missing records is: , 0)
         2255    (The amoung of missing records is: , 0)
         2256    (The amoung of missing records is: , 0)
         Length: 2257, dtype: object
\end{Verbatim}
            
    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    We have our function to check for missing records, now let us do
something mischievous and insert some dummy data into the dataframe and
test the reliability of our function. This dummy data is intended to
corrupt the dataset. I mean this happens a lot today, especially when
hackers want to hijack or corrupt a database.

We will insert a \texttt{Series}, which is basically a ``one-dimensional
labeled array capable of holding data of any type (integer, string,
float, python objects, etc.). The axis labels are collectively called
index.'', into our current dataframe.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}33}]:} \PY{n}{dummy\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dummy\PYZus{}record}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{text}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{category}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} \PY{n}{dummy\PYZus{}series}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}34}]:} text        dummy\_record
         category               1
         dtype: object
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}35}]:} \PY{n}{result\PYZus{}with\PYZus{}series} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{dummy\PYZus{}series}\PY{p}{,} \PY{n}{ignore\PYZus{}index}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} \PY{c+c1}{\PYZsh{} check if the records was commited into result}
         \PY{n+nb}{len}\PY{p}{(}\PY{n}{result\PYZus{}with\PYZus{}series}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}36}]:} 2258
\end{Verbatim}
            
    Now we that we have added the record with some missing values. Let try
our function and see if it can detect that there is a missing value on
the resulting dataframe.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{n}{result\PYZus{}with\PYZus{}series}\PY{o}{.}\PY{n}{isnull}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{dmh}\PY{o}{.}\PY{n}{check\PYZus{}missing\PYZus{}values}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}37}]:} text             (The amoung of missing records is: , 0)
         category         (The amoung of missing records is: , 0)
         category\_name    (The amoung of missing records is: , 1)
         dtype: object
\end{Verbatim}
            
    Indeed there is a missing value in this new dataframe. Specifically, the
missing value comes from the \texttt{category\_name} attribute. As I
mentioned before, there are many ways to conduct specific operations on
the dataframes. In this case let us use a simple dictionary and try to
insert it into our original dataframe \texttt{X}. Notice that above we
are not changing the \texttt{X} dataframe as results are directly
applied to the assignment variable provided. But in the event that we
just want to keep things simple, we can just directly apply the changes
to \texttt{X} and assign it to itself as we will do below. This
modification will create a need to remove this dummy record later on,
which means that we need to learn more about Pandas dataframes. This is
getting intense! But just relax, everything will be fine!

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} \PY{c+c1}{\PYZsh{} dummy record as dictionary format}
         \PY{n}{dummy\PYZus{}dict} \PY{o}{=} \PY{p}{[}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dummy\PYZus{}record}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                        \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{category}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{1}
                       \PY{p}{\PYZcb{}}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{n}{X} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{dummy\PYZus{}dict}\PY{p}{,} \PY{n}{ignore\PYZus{}index}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}40}]:} 2258
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{n}{X}\PY{o}{.}\PY{n}{isnull}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{dmh}\PY{o}{.}\PY{n}{check\PYZus{}missing\PYZus{}values}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}41}]:} text             (The amoung of missing records is: , 0)
         category         (The amoung of missing records is: , 0)
         category\_name    (The amoung of missing records is: , 1)
         dtype: object
\end{Verbatim}
            
    So now that we can see that our data has missing values, we want to
remove the records with missing values. The code to drop the record with
missing that we just added, is the following:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}42}]:} \PY{n}{X}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} drop the missing value }
\end{Verbatim}


    \ldots{} and now let us test to see if we gotten rid of the records with
missing values.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}43}]:} \PY{n}{X}\PY{o}{.}\PY{n}{isnull}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{dmh}\PY{o}{.}\PY{n}{check\PYZus{}missing\PYZus{}values}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}43}]:} text             (The amoung of missing records is: , 0)
         category         (The amoung of missing records is: , 0)
         category\_name    (The amoung of missing records is: , 0)
         dtype: object
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}44}]:} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}44}]:} 2257
\end{Verbatim}
            
    And we are back with our original dataset, clean and tidy as we want it.
That's enough on how to deal with missing values, let us now move unto
something more fun.

    But just in case you want to learn more about how to deal with missing
data, refer to the official
\href{http://pandas.pydata.org/pandas-docs/stable/missing_data.html\#missing-data}{Pandas
documentation}.

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{exercise-5-take-home}{%
\subsubsection{\texorpdfstring{\textgreater\textgreater\textgreater{}
\textbf{Exercise 5 (take
home)}}{\textgreater\textgreater\textgreater{} Exercise 5 (take home)}}\label{exercise-5-take-home}}

There is an old saying that goes, ``The devil is in the details.'' When
we are working with extremely large data, it's difficult to check
records one by one (as we have been doing so far). And also, we don't
even know what kind of missing values we are facing. Thus, ``debugging''
skills get sharper as we spend more time solving bugs. Let's focus on a
different method to check for missing values and the kinds of missing
values you may encounter. It's not easy to check for missing values as
you will find out in a minute.

Please check the data and the process below, describe what you observe
and why it happened.\\
\(Hint\) : why \texttt{.isnull()} didn't work?

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}45}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
         
         \PY{n}{NA\PYZus{}dict} \PY{o}{=} \PY{p}{[}\PY{p}{\PYZob{}} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{missing\PYZus{}example}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{nan} \PY{p}{\PYZcb{}}\PY{p}{,}
                    \PY{p}{\PYZob{}} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{B}\PY{l+s+s1}{\PYZsq{}}                    \PY{p}{\PYZcb{}}\PY{p}{,}
                    \PY{p}{\PYZob{}} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{missing\PYZus{}example}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{NaN}\PY{l+s+s1}{\PYZsq{}}  \PY{p}{\PYZcb{}}\PY{p}{,}
                    \PY{p}{\PYZob{}} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{D}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{missing\PYZus{}example}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{None}\PY{l+s+s1}{\PYZsq{}} \PY{p}{\PYZcb{}}\PY{p}{,}
                    \PY{p}{\PYZob{}} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{E}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{missing\PYZus{}example}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}  \PY{k+kc}{None}  \PY{p}{\PYZcb{}}\PY{p}{,}
                    \PY{p}{\PYZob{}} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{F}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{missing\PYZus{}example}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}     \PY{p}{\PYZcb{}}\PY{p}{]}
         
         \PY{n}{NA\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{NA\PYZus{}dict}\PY{p}{,} \PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{missing\PYZus{}example}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{NA\PYZus{}df}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}45}]:}   id missing\_example
         0  A             NaN
         1  B             NaN
         2  C             NaN
         3  D            None
         4  E            None
         5  F                
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}46}]:} \PY{n}{NA\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{missing\PYZus{}example}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{isnull}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}46}]:} 0     True
         1     True
         2    False
         3    False
         4     True
         5    False
         Name: missing\_example, dtype: bool
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}47}]:} \PY{c+c1}{\PYZsh{} Answer here}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{dealing-with-duplicate-data}{%
\subsubsection{4.2 Dealing with Duplicate
Data}\label{dealing-with-duplicate-data}}

Dealing with duplicate data is just as painful as dealing with missing
data. The worst case is that you have duplicate data that has missing
values. But let us not get carried away. Let us stick with the basics.
As we have learned in our Data Mining course, duplicate data can occur
because of many reasons. The majority of the times it has to do with how
we store data or how we collect and merge data. For instance, we may
have collected and stored a tweet, and a retweet of that same tweet as
two different records; this results in a case of data duplication; the
only difference being that one is the original tweet and the other the
retweeted one. Here you will learn that dealing with duplicate data is
not as challenging as missing values. But this also all depends on what
you consider as duplicate data, i.e., this all depends on your criteria
for what is considered as a duplicate record and also what type of data
you are dealing with. For textual data, it may not be so trivial as it
is for numerical values or images. Anyhow, let us look at some code on
how to deal with duplicate records in our \texttt{X} dataframe.

    First, let us check how many duplicates we have in our current dataset.
Here is the line of code that checks for duplicates; it is very similar
to the \texttt{isnull} function that we used to check for missing
values.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}48}]:} \PY{n}{X}\PY{o}{.}\PY{n}{duplicated}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}48}]:} 0       False
         1       False
         2       False
         3       False
         4       False
                 {\ldots}  
         2252    False
         2253    False
         2254    False
         2255    False
         2256    False
         Length: 2257, dtype: bool
\end{Verbatim}
            
    We can also check the sum of duplicate records by simply doing:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}49}]:} \PY{n+nb}{sum}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{duplicated}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}49}]:} 0
\end{Verbatim}
            
    Based on that output, you may be asking why did the \texttt{duplicated}
operation only returned one single column that indicates whether there
is a duplicate record or not. So yes, all the \texttt{duplicated()}
operation does is to check per records instead of per column. That is
why the operation only returns one value instead of three values for
each column. It appears that we don't have any duplicates since none of
our records resulted in \texttt{True}. If we want to check for
duplicates as we did above for some particular column, instead of all
columns, we do something as shown below. As you may have noticed, in the
case where we select some columns instead of checking by all columns, we
are kind of lowering the criteria of what is considered as a duplicate
record. So let us only check for duplicates by onyl checking the
\texttt{text} attribute.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}50}]:} \PY{n+nb}{sum}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{duplicated}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}50}]:} 0
\end{Verbatim}
            
    Now let us create some duplicated dummy records and append it to the
main dataframe \texttt{X}. Subsequenlty, let us try to get rid of the
duplicates.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}51}]:} \PY{n}{dummy\PYZus{}duplicate\PYZus{}dict} \PY{o}{=} \PY{p}{[}\PY{p}{\PYZob{}}
                                      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dummy record}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{category}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{,} 
                                      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{category\PYZus{}name}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dummy category}\PY{l+s+s2}{\PYZdq{}}
                                 \PY{p}{\PYZcb{}}\PY{p}{,}
                                 \PY{p}{\PYZob{}}
                                      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dummy record}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{category}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{,} 
                                      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{category\PYZus{}name}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dummy category}\PY{l+s+s2}{\PYZdq{}}
                                 \PY{p}{\PYZcb{}}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}52}]:} \PY{n}{X} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{dummy\PYZus{}duplicate\PYZus{}dict}\PY{p}{,} \PY{n}{ignore\PYZus{}index}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}53}]:} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}53}]:} 2259
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}54}]:} \PY{n+nb}{sum}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{duplicated}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}54}]:} 1
\end{Verbatim}
            
    We have added the dummy duplicates to \texttt{X}. Now we are faced with
the decision as to what to do with the duplicated records after we have
found it. In our case, we want to get rid of all the duplicated records
without preserving a copy. We can simply do that with the following line
of code:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}55}]:} \PY{n}{X}\PY{o}{.}\PY{n}{drop\PYZus{}duplicates}\PY{p}{(}\PY{n}{keep}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)} \PY{c+c1}{\PYZsh{} inplace applies changes directly on our dataframe}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}56}]:} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}56}]:} 2257
\end{Verbatim}
            
    Check out the Pandas
\href{http://pandas.pydata.org/pandas-docs/stable/indexing.html?highlight=duplicate\#duplicate-data}{documentation}
for more information on dealing with duplicate data.

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{data-preprocessing}{%
\subsection{5. Data Preprocessing}\label{data-preprocessing}}

In the Data Mining course we learned about the many ways of performing
data preprocessing. In reality, the list is quiet general as the
specifics of what data preprocessing involves is too much to cover in
one course. This is especially true when you are dealing with
unstructured data, as we are dealing with in this particular notebook.
But let us look at some examples for each data preprocessing technique
that we learned in the class. We will cover each item one by one, and
provide example code for each category. You will learn how to peform
each of the operations, using Pandas, that cover the essentials to
Preprocessing in Data Mining. We are not going to follow any strict
order, but the items we will cover in the preprocessing section of this
notebook are as follows:

\begin{itemize}
\tightlist
\item
  Aggregation
\item
  Sampling
\item
  Dimensionality Reduction
\item
  Feature Subset Selection
\item
  Feature Creation
\item
  Discretization and Binarization
\item
  Attribute Transformation
\end{itemize}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{sampling}{%
\subsubsection{5.1 Sampling}\label{sampling}}

The first concept that we are going to cover from the above list is
sampling. Sampling refers to the technique used for selecting data. The
functionalities that we use to selected data through queries provided by
Pandas are actually basic methods for sampling. The reasons for sampling
are sometimes due to the size of data -- we want a smaller subset of the
data that is still representatitive enough as compared to the original
dataset.

We don't have a problem of size in our current dataset since it is just
a couple thousand records long. But if we pay attention to how much
content is included in the \texttt{text} field of each of those records,
you will realize that sampling may not be a bad idea after all. In fact,
we have already done some sampling by just reducing the records we are
using here in this notebook; remember that we are only using four
categories from the all the 20 categories available. Let us get an idea
on how to sample using pandas operations.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}57}]:} \PY{n}{X\PYZus{}sample} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{n}{n}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{)} \PY{c+c1}{\PYZsh{}random state}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}58}]:} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}sample}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}58}]:} 1000
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}59}]:} \PY{n}{X\PYZus{}sample}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{4}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}59}]:}                                                    text  category  \textbackslash{}
         331   From: simon@monu6.cc.monash.edu.au Subject: Sa{\ldots}         3   
         1378  From: rah13@cunixb.cc.columbia.edu (Robert A H{\ldots}         1   
         2073  From: uk02183@nx10.mik.uky.edu (bryan k willia{\ldots}         1   
         1520  From: jacquier@gsbux1.uchicago.edu (Eric Jacqu{\ldots}         2   
         
                        category\_name  
         331   soc.religion.christian  
         1378           comp.graphics  
         2073           comp.graphics  
         1520                 sci.med  
\end{Verbatim}
            
    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{exercise-6-take-home}{%
\subsubsection{\textgreater\textgreater\textgreater{} Exercise 6 (take
home):}\label{exercise-6-take-home}}

Notice any changes to the \texttt{X} dataframe? What are they? Report
every change you noticed as compared to the previous state of
\texttt{X}. Feel free to query and look more closely at the dataframe
for these changes.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}60}]:} \PY{c+c1}{\PYZsh{} Answer here}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    Let's do something cool here while we are working with sampling! Let us
look at the distribution of categories in both the sample and original
dataset. Let us visualize and analyze the disparity between the two
datasets. To generate some visualizations, we are going to use
\texttt{matplotlib} python library. With matplotlib, things are faster
and compatability-wise it may just be the best visualization library for
visualizing content extracted from dataframes and when using Jupyter
notebooks. Let's take a loot at the magic of \texttt{matplotlib} below.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}61}]:} \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
         \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}62}]:} \PY{n}{categories}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}62}]:} ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}63}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{category\PYZus{}name}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} plot barchart for X\PYZus{}sample}
         \PY{n}{X}\PY{o}{.}\PY{n}{category\PYZus{}name}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{kind} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bar}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                             \PY{n}{title} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Category distribution}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                             \PY{n}{ylim} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{650}\PY{p}{]}\PY{p}{,}        
                                             \PY{n}{rot} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{11}\PY{p}{,} \PY{n}{figsize} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
soc.religion.christian    599
sci.med                   594
comp.graphics             584
alt.atheism               480
Name: category\_name, dtype: int64

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}63}]:} <matplotlib.axes.\_subplots.AxesSubplot at 0x7fabcff57908>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_137_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}64}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{X\PYZus{}sample}\PY{o}{.}\PY{n}{category\PYZus{}name}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} plot barchart for X\PYZus{}sample}
         \PY{n}{X\PYZus{}sample}\PY{o}{.}\PY{n}{category\PYZus{}name}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{kind} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bar}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                                    \PY{n}{title} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Category distribution}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                                    \PY{n}{ylim} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{300}\PY{p}{]}\PY{p}{,} 
                                                    \PY{n}{rot} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{12}\PY{p}{,} \PY{n}{figsize} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
soc.religion.christian    277
comp.graphics             268
sci.med                   259
alt.atheism               196
Name: category\_name, dtype: int64

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}64}]:} <matplotlib.axes.\_subplots.AxesSubplot at 0x7fabcfea00f0>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_138_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    You can use following command to see other available styles to prettify
your charts. \texttt{python\ print(plt.style.available)}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{exercise-7-5-min}{%
\subsubsection{\texorpdfstring{\textgreater\textgreater\textgreater{}
\textbf{Exercise 7 (5
min):}}{\textgreater\textgreater\textgreater{} Exercise 7 (5 min):}}\label{exercise-7-5-min}}

Notice that for the \texttt{ylim} parameters we hardcoded the maximum
value for y. Is it possible to automate this instead of hard-coding it?
How would you go about doing that? (Hint: look at code above for clues)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}65}]:} \PY{c+c1}{\PYZsh{} Answer here}
         
         \PY{c+c1}{\PYZsh{} plot barchart for X\PYZus{}sample}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{exercise-8-take-home}{%
\subsubsection{\texorpdfstring{\textgreater\textgreater\textgreater{}
\textbf{Exercise 8 (take
home):}}{\textgreater\textgreater\textgreater{} Exercise 8 (take home):}}\label{exercise-8-take-home}}

We can also do a side-by-side comparison of the distribution between the
two datasets, but maybe you can try that as an excerise. Below we show
you an snapshot of the type of chart we are looking for.

    \begin{figure}
\centering
\includegraphics{https://i.imgur.com/9eO431H.png}
\caption{alt txt}
\end{figure}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}66}]:} \PY{c+c1}{\PYZsh{} Answer here}
\end{Verbatim}


    

    One thing that stood out from the both datasets, is that the
distribution of the categories remain relatively the same, which is a
good sign for us data scientist. There are many ways to conduct sampling
on the dataset and still obtain a representative enough dataset. That is
not the main focus in this notebook, but if you would like to know more
about sampling and how the \texttt{sample} feature works, just reference
the Pandas documentation and you will find interesting ways to conduct
more advanced sampling.

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{feature-creation}{%
\subsubsection{5.2 Feature Creation}\label{feature-creation}}

The other operation from the list above that we are going to practise on
is the so-called feature creation. As the name suggests, in feature
creation we are looking at creating new interesting and useful features
from the original dataset; a feature which captures the most important
information from the raw information we already have access to. In our
\texttt{X} table, we would like to create some features from the
\texttt{text} field, but we are still not sure what kind of features we
want to create. We can think of an interesting problem we want to solve,
or something we want to analyze from the data, or some questions we want
to answer. This is one process to come up with features -- this process
is usually called \texttt{feature\ engineering} in the data science
community.

We know what feature creation is so let us get real involved with our
dataset and make it more interesting by adding some special features or
attributes if you will. First, we are going to obtain the
\textbf{unigrams} for each text. (Unigram is just a fancy word we use in
Text Mining which stands for `tokens' or `individual words'.) Yes, we
want to extract all the words found in each text and append it as a new
feature to the pandas dataframe. The reason for extracting unigrams is
not so clear yet, but we can start to think of obtaining some statistics
about the articles we have: something like \textbf{word distribution} or
\textbf{word frequency}.

Before going into any further coding, we will also introduce a useful
text mining library called \href{http://www.nltk.org/}{NLTK}. The NLTK
library is a natural language processing tool used for text mining
tasks, so might as well we start to familiarize ourselves with it from
now (It may come in handy for the final project!). In partcular, we are
going to use the NLTK library to conduct tokenization because we are
interested in splitting a sentence into its individual components, which
we refer to as words, emojis, emails, etc. So let us go for it! We can
call the \texttt{nltk} library as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ nltk}
\end{Highlighting}
\end{Shaded}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}67}]:} \PY{k+kn}{import} \PY{n+nn}{nltk}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}68}]:} \PY{c+c1}{\PYZsh{} takes a like a minute or two to process}
         \PY{n}{X}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{unigrams}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{dmh}\PY{o}{.}\PY{n}{tokenize\PYZus{}text}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}69}]:} \PY{n}{X}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{unigrams}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}69}]:} 0    [From, :, sd345, @, city.ac.uk, (, Michael, Co{\ldots}
         1    [From, :, ani, @, ms.uky.edu, (, Aniruddha, B{\ldots}
         2    [From, :, djohnson, @, cs.ucsd.edu, (, Darin, {\ldots}
         3    [From, :, s0612596, @, let.rug.nl, (, M.M, ., {\ldots}
         Name: unigrams, dtype: object
\end{Verbatim}
            
    If you take a closer look at the \texttt{X} table now, you will see the
new columns \texttt{unigrams} that we have added. You will notice that
it contains an array of tokens, which were extracted from the original
\texttt{text} field. At first glance, you will notice that the tokenizer
is not doing a great job, let us take a closer at a single record and
see what was the exact result of the tokenization using the
\texttt{nltk} library.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}70}]:} \PY{n}{X}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{4}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}70}]:}                                                 text  category  \textbackslash{}
         0  From: sd345@city.ac.uk (Michael Collier) Subje{\ldots}         1   
         1  From: ani@ms.uky.edu (Aniruddha B. Deglurkar) {\ldots}         1   
         2  From: djohnson@cs.ucsd.edu (Darin Johnson) Sub{\ldots}         3   
         3  From: s0612596@let.rug.nl (M.M. Zwart) Subject{\ldots}         3   
         
                     category\_name                                           unigrams  
         0           comp.graphics  [From, :, sd345, @, city.ac.uk, (, Michael, Co{\ldots}  
         1           comp.graphics  [From, :, ani, @, ms.uky.edu, (, Aniruddha, B{\ldots}  
         2  soc.religion.christian  [From, :, djohnson, @, cs.ucsd.edu, (, Darin, {\ldots}  
         3  soc.religion.christian  [From, :, s0612596, @, let.rug.nl, (, M.M, ., {\ldots}  
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}71}]:} \PY{n+nb}{list}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{unigrams}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}71}]:} [['From',
           ':',
           'sd345',
           '@',
           'city.ac.uk',
           '(',
           'Michael',
           'Collier',
           ')',
           'Subject',
           ':',
           'Converting',
           'images',
           'to',
           'HP',
           'LaserJet',
           'III',
           '?',
           'Nntp-Posting-Host',
           ':',
           'hampton',
           'Organization',
           ':',
           'The',
           'City',
           'University',
           'Lines',
           ':',
           '14',
           'Does',
           'anyone',
           'know',
           'of',
           'a',
           'good',
           'way',
           '(',
           'standard',
           'PC',
           'application/PD',
           'utility',
           ')',
           'to',
           'convert',
           'tif/img/tga',
           'files',
           'into',
           'LaserJet',
           'III',
           'format',
           '.',
           'We',
           'would',
           'also',
           'like',
           'to',
           'do',
           'the',
           'same',
           ',',
           'converting',
           'to',
           'HPGL',
           '(',
           'HP',
           'plotter',
           ')',
           'files',
           '.',
           'Please',
           'email',
           'any',
           'response',
           '.',
           'Is',
           'this',
           'the',
           'correct',
           'group',
           '?',
           'Thanks',
           'in',
           'advance',
           '.',
           'Michael',
           '.',
           '--',
           'Michael',
           'Collier',
           '(',
           'Programmer',
           ')',
           'The',
           'Computer',
           'Unit',
           ',',
           'Email',
           ':',
           'M.P.Collier',
           '@',
           'uk.ac.city',
           'The',
           'City',
           'University',
           ',',
           'Tel',
           ':',
           '071',
           '477-8000',
           'x3769',
           'London',
           ',',
           'Fax',
           ':',
           '071',
           '477-8565',
           'EC1V',
           '0HB',
           '.']]
\end{Verbatim}
            
    The \texttt{nltk} library does a pretty decent job of tokenizing our
text. There are many other tokenizers online, such as
\href{https://spacy.io/}{spaCy}, and the built in libraries provided by
\href{http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html}{scikit-learn}.
We are making use of the NLTK library because it is open source and
because it does a good job of segmentating text-based data.

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{feature-subset-selection}{%
\subsubsection{5.3 Feature subset
selection}\label{feature-subset-selection}}

Okay, so we are making some headway here. Let us now make things a bit
more interesting. We are going to do something different from what we
have been doing thus far. We are going use a bit of everything that we
have learned so far. Briefly speaking, we are going to move away from
our main dataset (one form of feature subset selection), and we are
going to generate a document-term matrix from the original dataset. In
other words we are going to be creating something like this.

    \begin{figure}
\centering
\includegraphics{https://docs.google.com/drawings/d/e/2PACX-1vS01RrtPHS3r1Lf8UjX4POgDol-lVF4JAbjXM3SAOU-dOe-MqUdaEMWwJEPk9TtiUvcoSqTeE--lNep/pub?w=748\&h=366}
\caption{alt txt}
\end{figure}

    Initially, it won't have the same shape as the table above, but we will
get into that later. For now, let us use scikit learn built in
functionalities to generate this document. You will see for yourself how
easy it is to generate this table without much coding.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}72}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{feature\PYZus{}extraction}\PY{n+nn}{.}\PY{n+nn}{text} \PY{k}{import} \PY{n}{CountVectorizer}
         
         \PY{n}{count\PYZus{}vect} \PY{o}{=} \PY{n}{CountVectorizer}\PY{p}{(}\PY{p}{)}
         \PY{n}{X\PYZus{}counts} \PY{o}{=} \PY{n}{count\PYZus{}vect}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{text}\PY{p}{)}
\end{Verbatim}


    What we did with those two lines of code is that we transorfmed the
articles into a \textbf{term-document matrix}. Those lines of code
tokenize each article using a built-in, default tokenizer (often
referred to as an \texttt{analzyer}) and then produces the word
frequency vector for each document. We can create our own analyzers or
even use the nltk analyzer that we previously built. To keep things tidy
and minimal we are going to use the default analyzer provided by
\texttt{CountVectorizer}. Let us look closely at this analyzer.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}73}]:} \PY{n}{analyze} \PY{o}{=} \PY{n}{count\PYZus{}vect}\PY{o}{.}\PY{n}{build\PYZus{}analyzer}\PY{p}{(}\PY{p}{)}
         \PY{n}{analyze}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Hello World!}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}\PYZdq{} \PYZdq{}.join(list(X[4:5].text))}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}73}]:} ['hello', 'world']
\end{Verbatim}
            
    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{exercise-9-5-min}{%
\subsubsection{\texorpdfstring{\textbf{\textgreater\textgreater\textgreater{}
Exercise 9 (5
min):}}{\textgreater\textgreater\textgreater{} Exercise 9 (5 min):}}\label{exercise-9-5-min}}

Let's analyze the first record of our X dataframe with the new analyzer
we have just built. Go ahead try it!

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}74}]:} \PY{c+c1}{\PYZsh{} Answer here}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    Now let us look at the term-document matrix we built above.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}75}]:} \PY{c+c1}{\PYZsh{} We can check the shape of this matrix by:}
         \PY{n}{X\PYZus{}counts}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}75}]:} (2257, 35788)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}76}]:} \PY{c+c1}{\PYZsh{} We can obtain the feature names of the vectorizer, i.e., the terms}
         \PY{c+c1}{\PYZsh{} usually on the horizontal axis}
         \PY{n}{count\PYZus{}vect}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}76}]:} ['00',
          '000',
          '0000',
          '0000001200',
          '000005102000',
          '0001',
          '000100255pixel',
          '00014',
          '000406',
          '0007']
\end{Verbatim}
            
    \begin{figure}
\centering
\includegraphics{https://i.imgur.com/57gA1sd.png}
\caption{alt txt}
\end{figure}

    Above we can see the features found in the all the documents \texttt{X},
which are basically all the terms found in all the documents. As I said
earlier, the transformation is not in the pretty format (table) we saw
above -- the term-document matrix. We can do many things with the
\texttt{count\_vect} vectorizer and its transformation
\texttt{X\_counts}. You can find more information on other cool stuff
you can do with the
\href{http://scikit-learn.org/stable/modules/feature_extraction.html\#text-feature-extraction}{CountVectorizer}.

Now let us try to obtain something that is as close to the pretty table
I provided above. Before jumping into the code for doing just that, it
is important to mention that the reason for choosing the
\texttt{fit\_transofrm} for the \texttt{CountVectorizer} is that it
efficiently learns the vocabulary dictionary and returns a term-document
matrix.

In the next bit of code, we want to extract the first five articles and
transform them into document-term matrix, or in this case a
2-dimensional array. Here it goes.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}77}]:} \PY{n}{X}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}77}]:}                                                 text  category  \textbackslash{}
         0  From: sd345@city.ac.uk (Michael Collier) Subje{\ldots}         1   
         1  From: ani@ms.uky.edu (Aniruddha B. Deglurkar) {\ldots}         1   
         2  From: djohnson@cs.ucsd.edu (Darin Johnson) Sub{\ldots}         3   
         3  From: s0612596@let.rug.nl (M.M. Zwart) Subject{\ldots}         3   
         4  From: stanly@grok11.columbiasc.ncr.com (stanly{\ldots}         3   
         
                     category\_name                                           unigrams  
         0           comp.graphics  [From, :, sd345, @, city.ac.uk, (, Michael, Co{\ldots}  
         1           comp.graphics  [From, :, ani, @, ms.uky.edu, (, Aniruddha, B{\ldots}  
         2  soc.religion.christian  [From, :, djohnson, @, cs.ucsd.edu, (, Darin, {\ldots}  
         3  soc.religion.christian  [From, :, s0612596, @, let.rug.nl, (, M.M, ., {\ldots}  
         4  soc.religion.christian  [From, :, stanly, @, grok11.columbiasc.ncr.com{\ldots}  
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}78}]:} \PY{c+c1}{\PYZsh{} we convert from sparse array to normal array}
         \PY{n}{X\PYZus{}counts}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{100}\PY{p}{]}\PY{o}{.}\PY{n}{toarray}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}78}]:} array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
\end{Verbatim}
            
    As you can see the result is just this huge sparse matrix, which is
computationally intensive to generate and difficult to visualize. But we
can see that the fifth record, specifically, contains a \texttt{1} in
the beginning, which from our feature names we can deduce that this
article contains exactly one \texttt{00} term.

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{exercise-10-take-home}{%
\subsubsection{\texorpdfstring{\textbf{\textgreater\textgreater\textgreater{}
Exercise 10 (take
home):}}{\textgreater\textgreater\textgreater{} Exercise 10 (take home):}}\label{exercise-10-take-home}}

We said that the \texttt{1} at the beginning of the fifth record
represents the \texttt{00} term. Notice that there is another 1 in the
same record. Can you provide code that can verify what word this 1
represents from the vocabulary. Try to do this as efficient as possible.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}79}]:} \PY{c+c1}{\PYZsh{} Answer here}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    We can also use the vectorizer to generate word frequency vector for new
documents or articles. Let us try that below:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}80}]:} \PY{n}{count\PYZus{}vect}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Something completely new.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{toarray}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}80}]:} array([[0, 0, 0, {\ldots}, 0, 0, 0]])
\end{Verbatim}
            
    Now let us put a \texttt{00} in the document to see if it is detected as
we expect.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}81}]:} \PY{n}{count\PYZus{}vect}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{00 Something completely new.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{toarray}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}81}]:} array([[1, 0, 0, {\ldots}, 0, 0, 0]])
\end{Verbatim}
            
    Impressive, huh!

    To get you started in thinking about how to better analyze your data or
transformation, let us look at this nice little heat map of our
term-document matrix. It may come as a surpise to see the gems you can
mine when you start to look at the data from a different perspective.
Visualization are good for this reason.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}82}]:} \PY{c+c1}{\PYZsh{} first twenty features only}
         \PY{n}{plot\PYZus{}x} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{term\PYZus{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{count\PYZus{}vect}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{20}\PY{p}{]}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}83}]:} \PY{n}{plot\PYZus{}x}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}83}]:} ['term\_00',
          'term\_000',
          'term\_0000',
          'term\_0000001200',
          'term\_000005102000',
          'term\_0001',
          'term\_000100255pixel',
          'term\_00014',
          'term\_000406',
          'term\_0007',
          'term\_000usd',
          'term\_0010',
          'term\_001004',
          'term\_0010580b',
          'term\_001125',
          'term\_001200201pixel',
          'term\_0014',
          'term\_001642',
          'term\_00196',
          'term\_002']
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}84}]:} \PY{c+c1}{\PYZsh{} obtain document index}
         \PY{n}{plot\PYZus{}y} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{doc\PYZus{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{list}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{index}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{20}\PY{p}{]}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}85}]:} \PY{n}{plot\PYZus{}z} \PY{o}{=} \PY{n}{X\PYZus{}counts}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{20}\PY{p}{]}\PY{o}{.}\PY{n}{toarray}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    For the heat map, we are going to use another visualization library
called \texttt{seaborn}. It's built on top of matplotlib and closely
integrated with pandas data structures. One of the biggest advantages of
seaborn is that its default aesthetics are much more visually appealing
than matplotlib. See comparison below.

    \begin{figure}
\centering
\includegraphics{https://i.imgur.com/1isxmIV.png}
\caption{alt txt}
\end{figure}

    The other big advantage of seaborn is that seaborn has some built-in
plots that matplotlib does not support. Most of these can eventually be
replicated by hacking away at matplotlib, but they're not built in and
require much more effort to build.

So without further ado, let us try it now!

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}86}]:} \PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
         
         \PY{n}{df\PYZus{}todraw} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{plot\PYZus{}z}\PY{p}{,} \PY{n}{columns} \PY{o}{=} \PY{n}{plot\PYZus{}x}\PY{p}{,} \PY{n}{index} \PY{o}{=} \PY{n}{plot\PYZus{}y}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{9}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{)}\PY{p}{)}
         \PY{n}{ax} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{heatmap}\PY{p}{(}\PY{n}{df\PYZus{}todraw}\PY{p}{,}
                          \PY{n}{cmap}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{PuRd}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
                          \PY{n}{vmin}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{vmax}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{annot}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_194_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Check out more beautiful color palettes here:
https://python-graph-gallery.com/197-available-color-palettes-with-matplotlib/

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{exercise-11-take-home}{%
\subsubsection{\texorpdfstring{\textbf{\textgreater\textgreater\textgreater{}
Exercise 11 (take
home):}}{\textgreater\textgreater\textgreater{} Exercise 11 (take home):}}\label{exercise-11-take-home}}

From the chart above, we can see how sparse the term-document matrix is;
i.e., there is only one terms with frequency of \texttt{1} in the
subselection of the matrix. By the way, you may have noticed that we
only selected 20 articles and 20 terms to plot the histrogram. As an
excersise you can try to modify the code above to plot the entire
term-document matrix or just a sample of it. How would you do this
efficiently? Remember there is a lot of words in the vocab. Report below
what methods you would use to get a nice and useful visualization

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}87}]:} \PY{c+c1}{\PYZsh{} Answer here}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    The great thing about what we have done so far is that we now open doors
to new problems. Let us be optimistic. Even though we have the problem
of sparsity and a very high dimensional data, we are now closer to
uncovering wonders from the data. You see, the price you pay for the
hard work is worth it because now you are gaining a lot of knowledge
from what was just a list of what appeared to be irrelevant articles.
Just the fact that you can blow up the data and find out interesting
characteristics about the dataset in just a couple lines of code, is
something that truly inspires me to practise Data Science. That's the
motivation right there!

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{dimensionality-reduction}{%
\subsubsection{5.4 Dimensionality
Reduction}\label{dimensionality-reduction}}

Since we have just touched on the concept of sparsity most naturally the
problem of ``curse of dimentionality'' comes up. I am not going to get
into the full details of what dimensionality reduction is and what it is
good for just the fact that is an excellent technique for visualizing
data efficiently (please refer to notes for more information). All I can
say is that we are going to deal with the issue of sparsity with a few
lines of code. And we are going to try to visualize our data more
efficiently with the results.

We are going to make use of Principal Component Analysis to efficeintly
reduce the dimensions of our data, with the main goal of ``finding a
projection that captures the largest amount of variation in the data.''
This concept is important as it is very useful for visualizing and
observing the characteristics of our dataset.

    \href{http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html}{PCA
Algorithm}

\textbf{Input:} Raw term-vector matrix

\textbf{Output:} Projections

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}88}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k}{import} \PY{n}{PCA}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}89}]:} \PY{n}{X\PYZus{}reduced} \PY{o}{=} \PY{n}{PCA}\PY{p}{(}\PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}counts}\PY{o}{.}\PY{n}{toarray}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}90}]:} \PY{n}{X\PYZus{}reduced}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}90}]:} (2257, 2)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}91}]:} \PY{n}{categories}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}91}]:} ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}92}]:} \PY{n}{col} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{coral}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{black}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{m}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} plot}
         \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{25}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
         \PY{n}{ax} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{p}{)}
         
         \PY{k}{for} \PY{n}{c}\PY{p}{,} \PY{n}{category} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{col}\PY{p}{,} \PY{n}{categories}\PY{p}{)}\PY{p}{:}
             \PY{n}{xs} \PY{o}{=} \PY{n}{X\PYZus{}reduced}\PY{p}{[}\PY{n}{X}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{category\PYZus{}name}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{n}{category}\PY{p}{]}\PY{o}{.}\PY{n}{T}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
             \PY{n}{ys} \PY{o}{=} \PY{n}{X\PYZus{}reduced}\PY{p}{[}\PY{n}{X}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{category\PYZus{}name}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{n}{category}\PY{p}{]}\PY{o}{.}\PY{n}{T}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
            
             \PY{n}{ax}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{xs}\PY{p}{,} \PY{n}{ys}\PY{p}{,} \PY{n}{c} \PY{o}{=} \PY{n}{c}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{ax}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gray}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linestyle}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{X Label}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{Y Label}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_208_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    From the 2D visualization above, we can see a slight ``hint of
separation in the data''; i.e., they might have some special grouping by
category, but it is not immediately clear. The PCA was applied to the
raw frequencies and this is considered a very naive approach as some
words are not really unique to a document. Only categorizing by word
frequency is considered a ``bag of words'' approach. Later on in the
course you will learn about different approaches on how to create better
features from the term-vector matrix, such as term-frequency inverse
document frequency so-called TF-IDF.

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{exercise-12-take-home}{%
\subsubsection{\textgreater\textgreater\textgreater{} Exercise 12 (take
home):}\label{exercise-12-take-home}}

Please try to reduce the dimension to 3, and plot the result use 3-D
plot. Use at least 3 different angle (camera position) to check your
result and describe what you found.

\(Hint\): you can refer to Axes3D in the documentation.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}93}]:} \PY{c+c1}{\PYZsh{} Answer here}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{atrribute-transformation-aggregation}{%
\subsubsection{5.5 Atrribute Transformation /
Aggregation}\label{atrribute-transformation-aggregation}}

We can do other things with the term-vector matrix besides applying
dimensionalaity reduction technique to deal with sparsity problem. Here
we are going to generate a simple distribution of the words found in all
the entire set of articles. Intuitively, this may not make any sense,
but in data science sometimes we take some things for granted, and we
just have to explore the data first before making any premature
conclusions. On the topic of attribute transformation, we will take the
word distribution and put the distribution in a scale that makes it easy
to analyze patterns in the distrubution of words. Let us get into it!

    First, we need to compute these frequencies for each term in all
documents. Visually speaking, we are seeking to add values of the 2D
matrix, vertically; i.e., sum of each column. You can also refer to this
process as aggregation, which we won't explore further in this notebook
because of the type of data we are dealing with. But I believe you get
the idea of what that includes.

    \begin{figure}
\centering
\includegraphics{https://docs.google.com/drawings/d/e/2PACX-1vTMfs0zWsbeAl-wrpvyCcZqeEUf7ggoGkDubrxX5XtwC5iysHFukD6c-dtyybuHnYigiRWRlRk2S7gp/pub?w=750\&h=412}
\caption{alt txt}
\end{figure}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}94}]:} \PY{c+c1}{\PYZsh{} note this takes time to compute. You may want to reduce the amount of terms you want to compute frequencies for}
         \PY{n}{term\PYZus{}frequencies} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{X\PYZus{}counts}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{:}
             \PY{n}{term\PYZus{}frequencies}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n+nb}{sum}\PY{p}{(}\PY{n}{X\PYZus{}counts}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{j}\PY{p}{]}\PY{o}{.}\PY{n}{toarray}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}95}]:} \PY{n}{term\PYZus{}frequencies} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{X\PYZus{}counts}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}96}]:} \PY{n}{term\PYZus{}frequencies}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}96}]:} 134
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}97}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
         \PY{n}{g} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{barplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{n}{count\PYZus{}vect}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{300}\PY{p}{]}\PY{p}{,} 
                     \PY{n}{y}\PY{o}{=}\PY{n}{term\PYZus{}frequencies}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{300}\PY{p}{]}\PY{p}{)}
         \PY{n}{g}\PY{o}{.}\PY{n}{set\PYZus{}xticklabels}\PY{p}{(}\PY{n}{count\PYZus{}vect}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{300}\PY{p}{]}\PY{p}{,} \PY{n}{rotation} \PY{o}{=} \PY{l+m+mi}{90}\PY{p}{)}\PY{p}{;}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_220_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{exercise-13-take-home}{%
\subsubsection{\texorpdfstring{\textgreater\textgreater\textgreater{}
\textbf{Exercise 13 (take
home):}}{\textgreater\textgreater\textgreater{} Exercise 13 (take home):}}\label{exercise-13-take-home}}

If you want a nicer interactive visualization here, I would encourage
you try to install and use plotly to achieve this.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}98}]:} \PY{c+c1}{\PYZsh{} Answer here}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{exercise-14-take-home}{%
\subsubsection{\texorpdfstring{\textgreater\textgreater\textgreater{}
\textbf{Exercise 14 (take
home):}}{\textgreater\textgreater\textgreater{} Exercise 14 (take home):}}\label{exercise-14-take-home}}

The chart above contains all the vocabulary, and it's computationally
intensive to both compute and visualize. Can you efficiently reduce the
number of terms you want to visualize as an exercise.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}99}]:} \PY{c+c1}{\PYZsh{} Answer here}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{exercise-15-take-home}{%
\subsubsection{\texorpdfstring{\textgreater\textgreater\textgreater{}
\textbf{Exercise 15 (take
home):}}{\textgreater\textgreater\textgreater{} Exercise 15 (take home):}}\label{exercise-15-take-home}}

Additionally, you can attempt to sort the terms on the \texttt{x-axis}
by frequency instead of in alphabetical order. This way the
visualization is more meaninfgul and you will be able to observe the so
called \href{https://en.wikipedia.org/wiki/Long_tail}{long tail} (get
familiar with this term since it will appear a lot in data mining and
other statistics courses). see picture below

\begin{figure}
\centering
\includegraphics{https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Long_tail.svg/1000px-Long_tail.svg.png}
\caption{alt txt}
\end{figure}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}100}]:} \PY{c+c1}{\PYZsh{} Answer here}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    Since we already have those term frequencies, we can also transform the
values in that vector into the log distribution. All we need is to
import the \texttt{math} library provided by python and apply it to the
array of values of the term frequency vector. This is a typical example
of attribute transformation. Let's go for it. The log distribution is a
technique to visualize the term frequency into a scale that makes you
easily visualize the distribution in a more readable format. In other
words, the variations between the term frequencies are now easy to
observe. Let us try it out!

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}101}]:} \PY{k+kn}{import} \PY{n+nn}{math}
          \PY{n}{term\PYZus{}frequencies\PYZus{}log} \PY{o}{=} \PY{p}{[}\PY{n}{math}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{i}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{term\PYZus{}frequencies}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}102}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
          \PY{n}{g} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{barplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{n}{count\PYZus{}vect}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{300}\PY{p}{]}\PY{p}{,}
                          \PY{n}{y}\PY{o}{=}\PY{n}{term\PYZus{}frequencies\PYZus{}log}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{300}\PY{p}{]}\PY{p}{)}
          \PY{n}{g}\PY{o}{.}\PY{n}{set\PYZus{}xticklabels}\PY{p}{(}\PY{n}{count\PYZus{}vect}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{300}\PY{p}{]}\PY{p}{,} \PY{n}{rotation} \PY{o}{=} \PY{l+m+mi}{90}\PY{p}{)}\PY{p}{;}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_233_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Besides observing a complete transformation on the disrtibution, notice
the scale on the y-axis. The log distribution in our unsorted example
has no meaning, but try to properly sort the terms by their frequency,
and you will see an interesting effect. Go for it!

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{discretization-and-binarization}{%
\subsubsection{5.6 Discretization and
Binarization}\label{discretization-and-binarization}}

In this section we are going to discuss a very important
pre-preprocessing technique used to transform the data, specifically
categorical values, into a format that satisfies certain criteria
required by particular algorithms. Given our current original dataset,
we would like to transform one of the attributes,
\texttt{category\_name}, into four binary attributes. In other words, we
are taking the category name and replacing it with a \texttt{n}
asymmetric binary attributes. The logic behind this transformation is
discussed in detail in the recommended Data Mining text book (please
refer to it on page 58). People from the machine learning community also
refer to this transformation as one-hot encoding, but as you may become
aware later in the course, these concepts are all the same, we just have
different prefrence on how we refer to the concepts. Let us take a look
at what we want to achieve in code.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}103}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{preprocessing}\PY{p}{,} \PY{n}{metrics}\PY{p}{,} \PY{n}{decomposition}\PY{p}{,} \PY{n}{pipeline}\PY{p}{,} \PY{n}{dummy}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}104}]:} \PY{n}{mlb} \PY{o}{=} \PY{n}{preprocessing}\PY{o}{.}\PY{n}{LabelBinarizer}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}105}]:} \PY{n}{mlb}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{category}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}105}]:} LabelBinarizer(neg\_label=0, pos\_label=1, sparse\_output=False)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}106}]:} \PY{n}{mlb}\PY{o}{.}\PY{n}{classes\PYZus{}}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}106}]:} array([0, 1, 2, 3])
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}107}]:} \PY{n}{X}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bin\PYZus{}category}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{mlb}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{category}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}108}]:} \PY{n}{X}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{9}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}108}]:}                                                 text  category  \textbackslash{}
          0  From: sd345@city.ac.uk (Michael Collier) Subje{\ldots}         1   
          1  From: ani@ms.uky.edu (Aniruddha B. Deglurkar) {\ldots}         1   
          2  From: djohnson@cs.ucsd.edu (Darin Johnson) Sub{\ldots}         3   
          3  From: s0612596@let.rug.nl (M.M. Zwart) Subject{\ldots}         3   
          4  From: stanly@grok11.columbiasc.ncr.com (stanly{\ldots}         3   
          5  From: vbv@lor.eeap.cwru.edu (Virgilio (Dean) B{\ldots}         3   
          6  From: jodfishe@silver.ucs.indiana.edu (joseph {\ldots}         3   
          7  From: aldridge@netcom.com (Jacquelin Aldridge){\ldots}         2   
          8  From: geb@cs.pitt.edu (Gordon Banks) Subject: {\ldots}         2   
          
                      category\_name                                           unigrams  \textbackslash{}
          0           comp.graphics  [From, :, sd345, @, city.ac.uk, (, Michael, Co{\ldots}   
          1           comp.graphics  [From, :, ani, @, ms.uky.edu, (, Aniruddha, B{\ldots}   
          2  soc.religion.christian  [From, :, djohnson, @, cs.ucsd.edu, (, Darin, {\ldots}   
          3  soc.religion.christian  [From, :, s0612596, @, let.rug.nl, (, M.M, ., {\ldots}   
          4  soc.religion.christian  [From, :, stanly, @, grok11.columbiasc.ncr.com{\ldots}   
          5  soc.religion.christian  [From, :, vbv, @, lor.eeap.cwru.edu, (, Virgil{\ldots}   
          6  soc.religion.christian  [From, :, jodfishe, @, silver.ucs.indiana.edu,{\ldots}   
          7                 sci.med  [From, :, aldridge, @, netcom.com, (, Jacqueli{\ldots}   
          8                 sci.med  [From, :, geb, @, cs.pitt.edu, (, Gordon, Bank{\ldots}   
          
             bin\_category  
          0  [0, 1, 0, 0]  
          1  [0, 1, 0, 0]  
          2  [0, 0, 0, 1]  
          3  [0, 0, 0, 1]  
          4  [0, 0, 0, 1]  
          5  [0, 0, 0, 1]  
          6  [0, 0, 0, 1]  
          7  [0, 0, 1, 0]  
          8  [0, 0, 1, 0]  
\end{Verbatim}
            
    Take a look at the new attribute we have added to the \texttt{X} table.
You can see that the new attribute, which is called
\texttt{bin\_category}, contains an array of 0's and 1's. The \texttt{1}
is basically to indicate the position of the label or category we
binarized. If you look at the first two records, the one is places in
slot 2 in the array; this helps to indicate to any of the algorithms
which we are feeding this data to, that the record belong to that
specific category.

Attributes with \textbf{continuous values} also have strategies to
tranform the data; this is usually called \textbf{Discretization}
(please refer to the text book for more inforamation).

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{exercise-16-take-home}{%
\subsubsection{\texorpdfstring{\textgreater\textgreater\textgreater{}
\textbf{Exercise 16 (take
home):}}{\textgreater\textgreater\textgreater{} Exercise 16 (take home):}}\label{exercise-16-take-home}}

Try to generate the binarization using the \texttt{category\_name}
column instead. Does it work?

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}109}]:} \PY{c+c1}{\PYZsh{} Answer here}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{data-exploration}{%
\section{6. Data Exploration}\label{data-exploration}}

    Sometimes you need to take a peek at your data to understand the
relationships in your dataset. Here, we will focus in a similarity
example. Let's take 3 documents and compare them.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}118}]:} \PY{c+c1}{\PYZsh{} We retrieve 2 sentences for a random record, here, indexed at 50 and 100}
          \PY{n}{document\PYZus{}to\PYZus{}transform\PYZus{}1} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{n}{random\PYZus{}record\PYZus{}1} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{l+m+mi}{50}\PY{p}{]}
          \PY{n}{random\PYZus{}record\PYZus{}1} \PY{o}{=} \PY{n}{random\PYZus{}record\PYZus{}1}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
          \PY{n}{document\PYZus{}to\PYZus{}transform\PYZus{}1}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{random\PYZus{}record\PYZus{}1}\PY{p}{)}
          
          \PY{n}{document\PYZus{}to\PYZus{}transform\PYZus{}2} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{n}{random\PYZus{}record\PYZus{}2} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{l+m+mi}{100}\PY{p}{]}
          \PY{n}{random\PYZus{}record\PYZus{}2} \PY{o}{=} \PY{n}{random\PYZus{}record\PYZus{}2}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
          \PY{n}{document\PYZus{}to\PYZus{}transform\PYZus{}2}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{random\PYZus{}record\PYZus{}2}\PY{p}{)}
          
          \PY{n}{document\PYZus{}to\PYZus{}transform\PYZus{}3} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{n}{random\PYZus{}record\PYZus{}3} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{l+m+mi}{150}\PY{p}{]}
          \PY{n}{random\PYZus{}record\PYZus{}3} \PY{o}{=} \PY{n}{random\PYZus{}record\PYZus{}3}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
          \PY{n}{document\PYZus{}to\PYZus{}transform\PYZus{}3}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{random\PYZus{}record\PYZus{}3}\PY{p}{)}
\end{Verbatim}


    Let's look at our emails.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}119}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{document\PYZus{}to\PYZus{}transform\PYZus{}1}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{n}{document\PYZus{}to\PYZus{}transform\PYZus{}2}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{n}{document\PYZus{}to\PYZus{}transform\PYZus{}3}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
['From: ab@nova.cc.purdue.edu (Allen B) Subject: Re: TIFF: philosophical significance of 42 Organization: Purdue University Lines: 39  In article <prestonm.735400848@cs.man.ac.uk> prestonm@cs.man.ac.uk (Martin   Preston) writes: > Why not use the PD C library for reading/writing TIFF files? It took me a > good 20 minutes to start using them in your own app.  I certainly do use it whenever I have to do TIFF, and it usually works very well.  That\textbackslash{}'s not my point.  I\textbackslash{}'m >philosophically< opposed to it because of its complexity.  This complexity has led to some programs\textbackslash{}' poor TIFF writers making some very bizarre files, other programs\textbackslash{}' inability to load TIFF images (though they\textbackslash{}'ll save them, of course), and a general inability to interchange images between different environments despite the fact they all think they understand TIFF.  As the saying goes, "It\textbackslash{}'s not me I\textbackslash{}'m worried about- it\textbackslash{}'s all the >other<  assholes out there!"  I\textbackslash{}'ve had big trouble with misuse and abuse of TIFF over the years, and I chalk it all up to the immense (and unnecessary) complexity of the format.  In the words of the TIFF 5.0 spec, Appendix G, page G-1 (capitalized emphasis mine):  "The only problem with this sort of success is that TIFF was designed to be powerful and flexible, at the expense of simplicity.  It takes a fair amount of effort to handle all the options currently defined in this specification (PROBABLY NO APPLICATION DOES A COMPLETE JOB), and that is currently the only way you can be >sure< that you will be able to import any TIFF image, since there are so many image-generating applications out there now."   If a program (or worse all applications) can\textbackslash{}'t read >every< TIFF image, that means there are some it won\textbackslash{}'t- some that I might have to deal with.  Why would I want my images to be trapped in that format?  I don\textbackslash{}'t and neither should anyone who agrees with my reasoning- not that anyone does, of course! :-)  ab ']
['From: mathew <mathew@mantis.co.uk> Subject: Re: university violating separation of church/state? Organization: Mantis Consultants, Cambridge. UK. X-Newsreader: rusnews v1.01 Lines: 29  dmn@kepler.unh.edu ({\ldots}until kings become philosophers or philosophers become kings) writes: >      Recently, RAs have been ordered (and none have resisted or cared about > it apparently) to post a religious flyer entitled \_The Soul Scroll: Thoughts > on religion, spirituality, and matters of the soul\_ on the inside of bathroom > stall doors. (at my school, the University of New Hampshire) It is some sort > of newsletter assembled by a Hall Director somewhere on campus. It poses a > question about \textbackslash{}'spirituality\textbackslash{}' each issue, and solicits responses to be  > included in the next \textbackslash{}'issue.\textbackslash{}' It\textbackslash{}'s all pretty vague. I assume it\textbackslash{}'s put out > by a Christian, but they\textbackslash{}'re very careful not to mention Jesus or the bible. > I\textbackslash{}'ve heard someone defend it, saying "Well it doesn\textbackslash{}'t support any one religion. > " So what??? This is a STATE university, and as a strong supporter of the > separation of church and state, I was enraged. >  >      What can I do about this?  It sounds to me like it\textbackslash{}'s just SCREAMING OUT for parody.  Give a copy to your friendly neighbourhood SubGenius preacher; with luck, he\textbackslash{}'ll run it through the mental mincer and hand you back an outrageously offensive and gut-bustingly funny parody you can paste over the originals.  I can see it now:                                 The Stool Scroll          Thoughts on Religion, Spirituality, and Matters of the Colon                         (You can use this text to wipe)   mathew ']
['From: lfoard@hopper.virginia.edu (Lawrence C. Foard) Subject: Re: Assurance of Hell Organization: ITC/UVA Community Access UNIX/Internet Project Lines: 43  In article <Apr.20.03.01.19.1993.3755@geneva.rutgers.edu> REXLEX@fnal.fnal.gov writes: > >I dreamed that the great judgment morning had dawned, >     and the trumpet had blown. >I dreamed that the sinners had gathered for judgment >     before the white throne. >Oh what weeping and wailing as the lost were told of their fate. >They cried for the rock and the mountains. >They prayed, but their prayers were too late. >The soul that had put off salvation,  >"Not tonight I\textbackslash{}'ll get saved by and by. > No time now to think of {\ldots} religion,"  >Alas, he had found time to die. >And I saw a Great White Throne.  If I believed in the God of the bible I would be very fearful of making this statement. Doesn\textbackslash{}'t it say those who judge will be judged by the same measure?   >Now, some have protest by saying that the fear of hell is not good for >motivation, yet Jesus thought it was.  Paul thought it was.  Paul said,  >"Knowing therefore, the terror of the Lord, we persuade men."  A God who must motivate through fear is not a God worthy of worship. If the God Jesus spoke of did indeed exist he would not need hell to convince people to worship him.  >Today, too much of our evangelism is nothing but soft soap and some of >it is nothing but evangelical salesmanship.  We don\textbackslash{}'t tell people anymore, that >there\textbackslash{}'s such a thing as sin or that there\textbackslash{}'s such a place as hell.    It was the myth of hell that made me finally realize that the whole thing was untrue. If it hadn\textbackslash{}'t been for hell I would still be a believer today. The myth of hell made me realize that if there was a God that he was not the all knowing and all good God he claimed to be. Why should I take such a being at his word, even if there was evidence for his existance?  --  ------          Join the Pythagorean Reform Church!               . \textbackslash{}\textbackslash{}    /        Repent of your evil irrational numbers             . .  \textbackslash{}\textbackslash{}  /   and bean eating ways. Accept 10 into your heart!        . . .   \textbackslash{}\textbackslash{}/   Call the Pythagorean Reform Church BBS at 508-793-9568  . . . .      ']

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}120}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{binarize}
          
          \PY{c+c1}{\PYZsh{} Transform sentence with Vectorizers}
          \PY{n}{document\PYZus{}vector\PYZus{}count\PYZus{}1} \PY{o}{=} \PY{n}{count\PYZus{}vect}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{document\PYZus{}to\PYZus{}transform\PYZus{}1}\PY{p}{)}
          \PY{n}{document\PYZus{}vector\PYZus{}count\PYZus{}2} \PY{o}{=} \PY{n}{count\PYZus{}vect}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{document\PYZus{}to\PYZus{}transform\PYZus{}2}\PY{p}{)}
          \PY{n}{document\PYZus{}vector\PYZus{}count\PYZus{}3} \PY{o}{=} \PY{n}{count\PYZus{}vect}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{document\PYZus{}to\PYZus{}transform\PYZus{}3}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Binarize vecors to simplify: 0 for abscence, 1 for prescence}
          \PY{n}{document\PYZus{}vector\PYZus{}count\PYZus{}1\PYZus{}bin} \PY{o}{=} \PY{n}{binarize}\PY{p}{(}\PY{n}{document\PYZus{}vector\PYZus{}count\PYZus{}1}\PY{p}{)}
          \PY{n}{document\PYZus{}vector\PYZus{}count\PYZus{}2\PYZus{}bin} \PY{o}{=} \PY{n}{binarize}\PY{p}{(}\PY{n}{document\PYZus{}vector\PYZus{}count\PYZus{}2}\PY{p}{)}
          \PY{n}{document\PYZus{}vector\PYZus{}count\PYZus{}3\PYZus{}bin} \PY{o}{=} \PY{n}{binarize}\PY{p}{(}\PY{n}{document\PYZus{}vector\PYZus{}count\PYZus{}3}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} print}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Let}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{s take a look at the count vectors:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{n}{document\PYZus{}vector\PYZus{}count\PYZus{}1}\PY{o}{.}\PY{n}{todense}\PY{p}{(}\PY{p}{)}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{n}{document\PYZus{}vector\PYZus{}count\PYZus{}2}\PY{o}{.}\PY{n}{todense}\PY{p}{(}\PY{p}{)}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{n}{document\PYZus{}vector\PYZus{}count\PYZus{}3}\PY{o}{.}\PY{n}{todense}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Let's take a look at the count vectors:
[[0 0 0 {\ldots} 0 0 0]]
[[0 0 0 {\ldots} 0 0 0]]
[[0 0 0 {\ldots} 0 0 0]]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}122}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics}\PY{n+nn}{.}\PY{n+nn}{pairwise} \PY{k}{import} \PY{n}{cosine\PYZus{}similarity}
          
          \PY{c+c1}{\PYZsh{} Calculate Cosine Similarity}
          \PY{n}{cos\PYZus{}sim\PYZus{}count\PYZus{}1\PYZus{}2} \PY{o}{=} \PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{document\PYZus{}vector\PYZus{}count\PYZus{}1}\PY{p}{,} \PY{n}{document\PYZus{}vector\PYZus{}count\PYZus{}2}\PY{p}{,} \PY{n}{dense\PYZus{}output}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
          \PY{n}{cos\PYZus{}sim\PYZus{}count\PYZus{}1\PYZus{}3} \PY{o}{=} \PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{document\PYZus{}vector\PYZus{}count\PYZus{}1}\PY{p}{,} \PY{n}{document\PYZus{}vector\PYZus{}count\PYZus{}3}\PY{p}{,} \PY{n}{dense\PYZus{}output}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
          \PY{n}{cos\PYZus{}sim\PYZus{}count\PYZus{}1\PYZus{}1} \PY{o}{=} \PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{document\PYZus{}vector\PYZus{}count\PYZus{}1}\PY{p}{,} \PY{n}{document\PYZus{}vector\PYZus{}count\PYZus{}1}\PY{p}{,} \PY{n}{dense\PYZus{}output}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
          \PY{n}{cos\PYZus{}sim\PYZus{}count\PYZus{}2\PYZus{}2} \PY{o}{=} \PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{document\PYZus{}vector\PYZus{}count\PYZus{}2}\PY{p}{,} \PY{n}{document\PYZus{}vector\PYZus{}count\PYZus{}2}\PY{p}{,} \PY{n}{dense\PYZus{}output}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Print }
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Cosine Similarity using count bw 1 and 2: }\PY{l+s+si}{\PYZpc{}(x)f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}}\PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{x}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{n}{cos\PYZus{}sim\PYZus{}count\PYZus{}1\PYZus{}2}\PY{p}{\PYZcb{}}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Cosine Similarity using count bw 1 and 3: }\PY{l+s+si}{\PYZpc{}(x)f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}}\PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{x}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{n}{cos\PYZus{}sim\PYZus{}count\PYZus{}1\PYZus{}3}\PY{p}{\PYZcb{}}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Cosine Similarity using count bw 1 and 1: }\PY{l+s+si}{\PYZpc{}(x)f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}}\PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{x}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{n}{cos\PYZus{}sim\PYZus{}count\PYZus{}1\PYZus{}1}\PY{p}{\PYZcb{}}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Cosine Similarity using count bw 2 and 2: }\PY{l+s+si}{\PYZpc{}(x)f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}}\PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{x}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{n}{cos\PYZus{}sim\PYZus{}count\PYZus{}2\PYZus{}2}\PY{p}{\PYZcb{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Cosine Similarity using count bw 1 and 2: 0.608862
Cosine Similarity using count bw 1 and 3: 0.622050
Cosine Similarity using count bw 1 and 1: 1.000000
Cosine Similarity using count bw 2 and 2: 1.000000

    \end{Verbatim}

    As expected, cosine similarity between a sentence and itself is 1.
Between 2 entirely different sentences, it will be 0.

We can assume that we have the more common features in bthe documents 1
and 3 than in documents 1 and 2. This reflects indeed in a higher
similarity than that of sentences 1 and 3.

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{concluding-remarks}{%
\subsection{7. Concluding Remarks}\label{concluding-remarks}}

    Wow! We have come a long way! We can now call ourselves experts of Data
Preprocessing. You should feel excited and proud because the process of
Data Mining usually involves 70\% preprocessing and 30\% training
learning models. You will learn this as you progress in the Data Mining
course. I really feel that if you go through the exercises and challenge
yourself, you are on your way to becoming a super Data Scientist.

From here the possibilities for you are endless. You now know how to use
almost every common technique for preprocessing with state-of-the-art
tools, such as as Pandas and Scikit-learn. You are now with the trend!

After completing this notebook you can do a lot with the results we have
generated. You can train algorithms and models that are able to classify
articles into certain categories and much more. You can also try to
experiment with different datasets, or venture further into text
analytics by using new deep learning techniques such as word2vec. All of
this will be presented in the next lab session. Until then, go teach
machines how to be intelligent to make the world a better place.

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{references}{%
\subsection{. References}\label{references}}

    \begin{itemize}
\tightlist
\item
  Pandas cook book
  (\href{http://pandas.pydata.org/pandas-docs/stable/cookbook.html}{Recommended
  for starters})
\item
  \href{https://dl.acm.org/citation.cfm?id=1095618}{Pang-Ning Tan,
  Michael Steinbach, Vipin Kumar, Introduction to Data Mining, Addison
  Wesley}
\end{itemize}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
